% Induced demand
@Inbook{Speck2018,
  author="Speck, Jeff",
  title="Understand Induced Demand",
  bookTitle="Walkable City Rules: 101 Steps to Making Better Places",
  year="2018",
  publisher="Island Press/Center for Resource Economics",
  address="Washington, DC",
  pages="64--65",
  abstract="TRAFFIC ENGINEERING THEORY is straightforward: a street is congested because the number of drivers exceeds its capacity. If you enlarge the street, you will eliminate congestion. Unfortunately, seventy-five years of evidence tells us that this almost never happens. Instead, what happens is that the number of drivers quickly increases to match the increased capacity, and congestion returns in full force. It's called induced demand. These new drivers are the people who were taking transit, carpooling, commuting off-peak, or simply not driving because they didn't want to be stuck in traffic. When the traffic went away, they changed their habits. Maybe they even moved farther away from work, as the time-cost of their commute went down. Unfortunately, thanks to them and others like them, this honeymoon couldn't last long.",
  isbn="978-1-61091-899-2",
  doi="10.5822/978-1-61091-899-2_27",
  url="https://doi.org/10.5822/978-1-61091-899-2_27"
}


% OD Matrices
@article{bell1983estimation,
  title={The estimation of an origin-destination matrix from traffic counts},
  author={Bell, Michael GH},
  journal={Transportation Science},
  volume={17},
  number={2},
  pages={198--217},
  year={1983},
  publisher={INFORMS}
}

@online{ODMilano2010,
  author = {Agenzia Mobilita' Ambiente Territorio di Milano},
  title = {Milan 2010 origin/destination dataset},
  year = 2010,
  url = {https://www.amat-mi.it/it/servizi/verifiche-sostenibilita-trasportistica/},
}

@online{ODLombardia2014,
  author = {Regione Lombardia},
  title = {Lombardy 2014 origin/destination dataset},
  year = 2014,
  url = {https://www.dati.lombardia.it/Mobilit-e-trasporti/Matrice-OD2014-DISAGGREGATA-file-compresso-per-dow/rwsg-m4kj/about_data},
}

% ZTLs
@article{derobertis2016traffic,
  title={Traffic-Restricted Zones in Italy},
  author={DeRobertis, Michelle and Tira, M},
  journal={Institute of Transportation Engineers (Annual Report Issue)},
  volume={86},
  number={12},
  pages={44--49},
  year={2016}
}

% Vehicle characterization
@article{schreiter2013vehicle,
  author = {Thomas Schreiter and Ramon L. Landman and J. W. C. (Hans) van Lint and Andreas Hegyi and Serge P. Hoogendoorn},
  title ={Vehicle Class–Specific Route Guidance of Freeway Traffic by Model-Predictive Control},
  journal = {Transportation Research Record},
  volume = {2324},
  number = {1},
  pages = {53-62},
  year = {2012},
  doi = {10.3141/2324-07},
  URL = {https://doi.org/10.3141/2324-07
  },
  eprint = {https://doi.org/10.3141/2324-07},
}

@article{wang2012understanding,
  title={Understanding road usage patterns in urban areas},
  author={Wang, Pu and Hunter, Timothy and Bayen, Alexandre M and Schechtner, Katja and Gonz{\'a}lez, Marta C},
  journal={Scientific reports},
  volume={2},
  number={1},
  pages={1001},
  year={2012},
  publisher={Nature Publishing Group UK London}
}

%Simulators and things

@misc{sumorl,
    author = {Lucas N. Alegre},
    title = {{SUMO-RL}},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/LucasAlegre/sumo-rl}},
}

@inproceedings{10.1145/3308558.3314139,
  author = {Zhang, Huichu and Feng, Siyuan and Liu, Chang and Ding, Yaoyao and Zhu, Yichen and Zhou, Zihan and Zhang, Weinan and Yu, Yong and Jin, Haiming and Li, Zhenhui},
  title = {CityFlow: A Multi-Agent Reinforcement Learning Environment for Large Scale City Traffic Scenario},
  year = {2019},
  isbn = {9781450366748},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3308558.3314139},
  doi = {10.1145/3308558.3314139},
  booktitle = {The World Wide Web Conference},
  pages = {3620–3624},
  numpages = {5},
  keywords = {Microscopic Traffic Simulation, Mobility, Reinforcement Learning Platform},
  location = {San Francisco, CA, USA},
  series = {WWW '19}
}

@inproceedings{krajzewicz2002sumo,
  title={SUMO (Simulation of Urban MObility)-an open-source traffic simulation},
  author={Krajzewicz, Daniel and Hertkorn, Georg and R{\"o}ssel, Christian and Wagner, Peter},
  booktitle={Proceedings of the 4th middle East Symposium on Simulation and Modelling (MESM20002)},
  pages={183--187},
  year={2002}
}

% RL Algorithms

% REINFORCE
@Inbook{Sewak2019,
author="Sewak, Mohit",
title="Policy-Based Reinforcement Learning Approaches",
bookTitle="Deep Reinforcement Learning: Frontiers of Artificial Intelligence",
year="2019",
publisher="Springer Singapore",
address="Singapore",
pages="127--140",
isbn="978-981-13-8285-7",
doi="10.1007/978-981-13-8285-7_10",
url="https://doi.org/10.1007/978-981-13-8285-7_10"
}

% Epsilon Greedy
@article{liu2021improving,
  title={Improving ant colony optimization algorithm with epsilon greedy and Levy flight},
  author={Liu, Yahui and Cao, Buyang and Li, Hehua},
  journal={Complex \& Intelligent Systems},
  volume={7},
  number={4},
  pages={1711--1722},
  year={2021},
  publisher={Springer}
}

% Monte Carlo methods
@inproceedings{NIPS2014_88bf0c64,
 author = {Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard and Wang, Xiaoshi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/88bf0c64edabeeb913c378227beef8f9-Paper.pdf},
 volume = {27},
 year = {2014}
}

% Temporal Difference methods
@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

% Policy Gradient Methods
@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

% Actor Critic methods
@article{grondman2012survey,
  title={A survey of actor-critic reinforcement learning: Standard and natural policy gradients},
  author={Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel AD and Babuska, Robert},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, part C (applications and reviews)},
  volume={42},
  number={6},
  pages={1291--1307},
  year={2012},
  publisher={IEEE}
}

% Actor Critic diagram
@article{abdalla2023actor,
  title={Actor-critic reinforcement learning leads decision-making in energy systems optimization—steam injection optimization},
  author={Abdalla, Ramez and Hollstein, Wolfgang and Carvajal, Carlos Paz and Jaeger, Philip},
  journal={Neural Computing and Applications},
  volume={35},
  number={22},
  pages={16633--16647},
  year={2023},
  publisher={Springer}
}

% TRPO
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

