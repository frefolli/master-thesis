% Induced demand
@Inbook{Speck2018,
  author="Speck, Jeff",
  title="Understand Induced Demand",
  bookTitle="Walkable City Rules: 101 Steps to Making Better Places",
  year="2018",
  publisher="Island Press/Center for Resource Economics",
  address="Washington, DC",
  pages="64--65",
  abstract="TRAFFIC ENGINEERING THEORY is straightforward: a street is congested because the number of drivers exceeds its capacity. If you enlarge the street, you will eliminate congestion. Unfortunately, seventy-five years of evidence tells us that this almost never happens. Instead, what happens is that the number of drivers quickly increases to match the increased capacity, and congestion returns in full force. It's called induced demand. These new drivers are the people who were taking transit, carpooling, commuting off-peak, or simply not driving because they didn't want to be stuck in traffic. When the traffic went away, they changed their habits. Maybe they even moved farther away from work, as the time-cost of their commute went down. Unfortunately, thanks to them and others like them, this honeymoon couldn't last long.",
  isbn="978-1-61091-899-2",
  doi="10.5822/978-1-61091-899-2_27",
  url="https://doi.org/10.5822/978-1-61091-899-2_27"
}


% OD Matrices
@article{bell1983estimation,
  title={The estimation of an origin-destination matrix from traffic counts},
  author={Bell, Michael GH},
  journal={Transportation Science},
  volume={17},
  number={2},
  pages={198--217},
  year={1983},
  publisher={INFORMS}
}

@online{ODMilano2010,
  author = {Agenzia Mobilita' Ambiente Territorio di Milano},
  title = {Milan 2010 origin/destination dataset},
  year = 2010,
  url = {https://www.amat-mi.it/it/servizi/verifiche-sostenibilita-trasportistica/},
}

@online{ODLombardia2014,
  author = {Regione Lombardia},
  title = {Lombardy 2014 origin/destination dataset},
  year = 2014,
  url = {https://www.dati.lombardia.it/Mobilit-e-trasporti/Matrice-OD2014-DISAGGREGATA-file-compresso-per-dow/rwsg-m4kj/about_data},
}

% ZTLs
@article{derobertis2016traffic,
  title={Traffic-Restricted Zones in Italy},
  author={DeRobertis, Michelle and Tira, M},
  journal={Institute of Transportation Engineers (Annual Report Issue)},
  volume={86},
  number={12},
  pages={44--49},
  year={2016}
}

% Vehicle characterization
@article{schreiter2013vehicle,
  author = {Thomas Schreiter and Ramon L. Landman and J. W. C. (Hans) van Lint and Andreas Hegyi and Serge P. Hoogendoorn},
  title ={Vehicle Class–Specific Route Guidance of Freeway Traffic by Model-Predictive Control},
  journal = {Transportation Research Record},
  volume = {2324},
  number = {1},
  pages = {53-62},
  year = {2012},
  doi = {10.3141/2324-07},
  URL = {https://doi.org/10.3141/2324-07
  },
  eprint = {https://doi.org/10.3141/2324-07},
}

@article{wang2012understanding,
  title={Understanding road usage patterns in urban areas},
  author={Wang, Pu and Hunter, Timothy and Bayen, Alexandre M and Schechtner, Katja and Gonz{\'a}lez, Marta C},
  journal={Scientific reports},
  volume={2},
  number={1},
  pages={1001},
  year={2012},
  publisher={Nature Publishing Group UK London}
}

%Simulators and things

@misc{sumorl,
    author = {Lucas N. Alegre},
    title = {{SUMO-RL}},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/LucasAlegre/sumo-rl}},
}

@misc{sumorl2,
    author = {Francesco Refolli},
    title = {{SUMO-RL 2.0}},
    year = {2025},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/frefolli/sumo-rl}},
}

@inproceedings{10.1145/3308558.3314139,
  author = {Zhang, Huichu and Feng, Siyuan and Liu, Chang and Ding, Yaoyao and Zhu, Yichen and Zhou, Zihan and Zhang, Weinan and Yu, Yong and Jin, Haiming and Li, Zhenhui},
  title = {CityFlow: A Multi-Agent Reinforcement Learning Environment for Large Scale City Traffic Scenario},
  year = {2019},
  isbn = {9781450366748},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3308558.3314139},
  doi = {10.1145/3308558.3314139},
  booktitle = {The World Wide Web Conference},
  pages = {3620–3624},
  numpages = {5},
  keywords = {Microscopic Traffic Simulation, Mobility, Reinforcement Learning Platform},
  location = {San Francisco, CA, USA},
  series = {WWW '19}
}

@inproceedings{krajzewicz2002sumo,
  title={SUMO (Simulation of Urban MObility)-an open-source traffic simulation},
  author={Krajzewicz, Daniel and Hertkorn, Georg and R{\"o}ssel, Christian and Wagner, Peter},
  booktitle={Proceedings of the 4th middle East Symposium on Simulation and Modelling (MESM20002)},
  pages={183--187},
  year={2002}
}

% RL Algorithms

% REINFORCE
@Inbook{Sewak2019,
author="Sewak, Mohit",
title="Policy-Based Reinforcement Learning Approaches",
bookTitle="Deep Reinforcement Learning: Frontiers of Artificial Intelligence",
year="2019",
publisher="Springer Singapore",
address="Singapore",
pages="127--140",
isbn="978-981-13-8285-7",
doi="10.1007/978-981-13-8285-7_10",
url="https://doi.org/10.1007/978-981-13-8285-7_10"
}

% Epsilon Greedy
@article{liu2021improving,
  title={Improving ant colony optimization algorithm with epsilon greedy and Levy flight},
  author={Liu, Yahui and Cao, Buyang and Li, Hehua},
  journal={Complex \& Intelligent Systems},
  volume={7},
  number={4},
  pages={1711--1722},
  year={2021},
  publisher={Springer}
}

% Monte Carlo methods
@inproceedings{NIPS2014_88bf0c64,
 author = {Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard and Wang, Xiaoshi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/88bf0c64edabeeb913c378227beef8f9-Paper.pdf},
 volume = {27},
 year = {2014}
}

% Temporal Difference methods
@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

% Policy Gradient Methods
@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

% Actor Critic methods
@article{grondman2012survey,
  title={A survey of actor-critic reinforcement learning: Standard and natural policy gradients},
  author={Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel AD and Babuska, Robert},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, part C (applications and reviews)},
  volume={42},
  number={6},
  pages={1291--1307},
  year={2012},
  publisher={IEEE}
}

% Actor Critic diagram
@article{abdalla2023actor,
  title={Actor-critic reinforcement learning leads decision-making in energy systems optimization—steam injection optimization},
  author={Abdalla, Ramez and Hollstein, Wolfgang and Carvajal, Carlos Paz and Jaeger, Philip},
  journal={Neural Computing and Applications},
  volume={35},
  number={22},
  pages={16633--16647},
  year={2023},
  publisher={Springer}
}

% TRPO
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

% DQN
@article{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

% DQL

@article{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

% QL

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

% SARSA
@book{rummery1994line,
  title={On-line Q-learning using connectionist systems},
  author={Rummery, Gavin A and Niranjan, Mahesan},
  volume={37},
  year={1994},
  publisher={University of Cambridge, Department of Engineering Cambridge, UK}
}

% PPO

@InProceedings{pmlr-v115-wang20b,
  title = 	 {Truly Proximal Policy Optimization},
  author =       {Wang, Yuhui and He, Hao and Tan, Xiaoyang},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {113--122},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v115/wang20b/wang20b.pdf},
  url = 	 {https://proceedings.mlr.press/v115/wang20b.html},
}

% CartPole

@inproceedings{nagendra2017comparison,
  title={Comparison of reinforcement learning algorithms applied to the cart-pole problem},
  author={Nagendra, Savinay and Podila, Nikhil and Ugarakhod, Rashmi and George, Koshy},
  booktitle={2017 international conference on advances in computing, communications and informatics (ICACCI)},
  pages={26--32},
  year={2017},
  organization={IEEE}
}

% Agents learning creativity
@article{10.1162/artl_a_00319,
    author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Cheney, Nick and Chrabaszcz, Patryk and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fŕenoy, Antoine and Gagńe, Christian and Le Goff, Leni and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Schulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, François and Tarapore, Danesh and Thibault, Simon and Watson, Richard and Weimer, Westley and Yosinski, Jason},
    title = {The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities},
    journal = {Artificial Life},
    volume = {26},
    number = {2},
    pages = {274-306},
    year = {2020},
    month = {05},
    abstract = {Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: Artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes, uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This article is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
    issn = {1064-5462},
    doi = {10.1162/artl_a_00319},
    url = {https://doi.org/10.1162/artl\_a\_00319},
    eprint = {https://direct.mit.edu/artl/article-pdf/26/2/274/1896071/artl\_a\_00319.pdf},
}

% L2R
@inproceedings{akiyama2016learning,
  title={Learning evaluation function for decision making of soccer agents using learning to rank},
  author={Akiyama, Hidehisa and Tsuji, Masashi and Aramaki, Shigeto},
  booktitle={2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)},
  pages={239--242},
  year={2016},
  organization={IEEE}
}

% WORKS IN MY AREA

% Similar state to mine
% Concept of pressure
@inproceedings{wei2019presslight,
  title={Presslight: Learning max pressure control to coordinate traffic signals in arterial network},
  author={Wei, Hua and Chen, Chacha and Zheng, Guanjie and Wu, Kan and Gayah, Vikash and Xu, Kai and Li, Zhenhui},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1290--1298},
  year={2019}
}

% Similar rewards
@article{han2023leveraging,
  title={Leveraging reinforcement learning for dynamic traffic control: A survey and challenges for field implementation},
  author={Han, Yu and Wang, Meng and Leclercq, Ludovic},
  journal={Communications in Transportation Research},
  volume={3},
  pages={100104},
  year={2023},
  publisher={Elsevier}
}
