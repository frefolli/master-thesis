# Unconventional reinforcement learning on traffic lights with SUMO

## Thesis or Master Degree in Computer Science, academic year 2024/2025

## Index

- [ ] Introduction
- [x] Traffic Science
  - [x] Introduction
  - [x] Transit regulation
  - [x] Demand models
  - [x] Settings and assumptions
- [x] Traffic Simulation
  - [x] SUMO
  - [x] Multi agent transit simulations
  - [x] SUMO-RL
- [x] Background on Reinforcement Learning
  - [x] A new perspective
    - [x] Learning by example
    - [x] Learning by inference
    - [x] Learning by experience
  - [x] Mathematics foundation of Environment Modeling and Reinforcement Learning
    - [x] Markov Chains
    - [x] Markov Decision Processes
    - [x] Policies
    - [x] Value Functions
      - [x] State-value function
      - [x] Action-value function
      - [x] Usefulness of value functions
  - [x] Learning to learn
    - [x] Learning techniques
      - [x] Model-Based/Free learning
      - [x] On/Off-Policy learning
      - [x] Monte Carlo methods
      - [x] Temporal Difference methods
      - [x] Policy-gradient methods
      - [x] Actor-Critic methods
    - [x] Reinforcement Learning Algorithms
      - [x] SARSA
      - [x] Q Learning
      - [x] Double Q Learning
      - [x] DQN
      - [x] PPO
  - [x] Design considerations
    - [x] Environment design
      - [x] The state space
      - [x] The action space
      - [x] The observation function
      - [x] The reward function
    - [x] Experience design
      - [x] Incremental learning
      - [x] Curriculum learning
      - [x] Simulated learning
- [x] The Extended SUMO-RL Framework
  - [x] The Core
    - [x] The Architecture
    - [x] The Environment
      - [x] Loading the scenario
      - [x] Acquiring traffic light phases
      - [x] Running the simulation
    - [x] The Observation Functions
    - [x] The Reward Functions
    - [x] The Agents
    - [x] A Self-Adaptive algorithm
  - [x] The Tools
    - [x] Importing from external sources
      - [x] The AMAT format
      - [x] The CityFlow format
      - [x] The SUMO format
      - [x] The converters
    - [x] Generating traffic
      - [x] Computing network capacity
      - [x] The traffic types
      - [x] The traffic registry
        - [x] Normal
        - [x] Railway Strike
        - [x] Public Transit Strike
        - [x] Tramway Roadworks
      - [x] The traffic specifier
        - [x] $2,\pounds,N1,N2,N1,N3,N1$
        - [x] $1,400000,\pounds,*,\sim$
      - [x] Generating datasets
    - [x] Executing experiments
    - [x] Analyzing experiment runs
      - [x] The raw material
      - [x] Extracting metrics
      - [x] Comparing configurations
      - [x] Generating automated reports
      - [x] Visualizing raw metrics
- [ ] Experiments and findings
  - [ ] RQ 0 - Acquiring a baseline
    - [ ] E0: Find Best Reward Function
    - [ ] E1: Find Best Observation Function
  - [ ] RQ 1 - Curriculum vs Monolithic Learning
    - [ ] E2: Find The Best Dataset
  - [ ] RQ 2 - Deep vs Tabular Learning
    - [ ] E3A: Find The Best Tabular Agent
    - [ ] E3B: Find The Best Deep Agent
  - [ ] RQ 3 - RL vs Fixed vs Priority
    - [ ] E3C: Find The Best Fixed Agent
    - [ ] E7: Try Unattended
  - [ ] RQ 4 - Determinism Sensitivity
    - [ ] E11: Explore determinism (DQL)
    - [ ] E12: Explore determinism (PPO)
  - [ ] RQ 5 - Quantization Sensitivity
    - [ ] E8: Try Unquantized (DQL)
    - [ ] E9: Try Unquantized (PPO)
  - [ ] RQ 6 - Multi-Agent vs Single-Agent Learning
    - [ ] E5: Try Marl On Observation
    - [ ] E6: Try Marl On Reward
    - [ ] E10: Try Partition
  - [ ] RQ 7 - Static vs Self-Adaptive System
    - [ ] E4: Try Self Adaptive
- [ ] Threats to Validity
  - [ ] Internal threats
    - [ ] Simplistic simulation model
    - [ ] No advanced tweaks in RL Models
  - [ ] External threats
    - [ ] Tiny scenarios
    - [ ] No experimentation on wide networks with OD Matrix
- [ ] Future Work
  - [ ] Apply results to Via Bassini to lower waiting time for ATM trolleybus 93, my beloved
  - [ ] Apply results to Via Porto Corsini to catch ATM M1 in time for Bisceglie instead of Rho Fieramilano
- [ ] Conclusion
