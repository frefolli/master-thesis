# Unconventional reinforcement learning on traffic lights with SUMO

## Thesis or Master Degree in Computer Science, academic year 2024/2025

## Index

- [ ] Introduction
- [x] Traffic Science
  - [x] Introduction
  - [x] Transit regulation
  - [x] Demand models
  - [x] Settings and assumptions
- [x] Traffic Simulation
  - [x] SUMO
  - [x] Multi agent transit simulations
  - [x] SUMO-RL
- [ ] Background on Reinforcement Learning
  - [x] A new perspective
    - [x] Learning by example
    - [x] Learning by inference
    - [x] Learning by experience
  - [x] Mathematics foundation of Environment Modeling and Reinforcement Learning
    - [x] Markov Chains
    - [x] Markov Decision Processes
    - [x] Policies
    - [x] Value Functions
      - [x] State-value function
      - [x] Action-value function
      - [x] Usefulness of value functions
  - [x] Learning to learn
    - [x] Learning techniques
      - [x] Model-Based/Free learning
      - [x] On/Off-Policy learning
      - [x] Monte Carlo methods
      - [x] Temporal Difference methods
      - [x] Policy-gradient methods
      - [x] Actor-Critic methods
    - [x] Reinforcement Learning Algorithms
      - [x] SARSA
      - [x] Q Learning
      - [x] Double Q Learning
      - [x] DQN
      - [x] PPO
  - [x] Design considerations
    - [x] Environment design
      - [x] The state space
      - [x] The action space
      - [x] The observation function
      - [x] The reward function
    - [x] Experience design
      - [x] Incremental learning
      - [x] Curriculum learning
      - [x] Simulated learning
- [ ] Agents
  - [ ] Q Learning
  - [ ] DQN
  - [ ] PPO
  - [ ] Fixed Cycle
- [ ] Fasteners and tools
  - [ ] Observation functions
  - [ ] Reward functions
  - [ ] Traffic generation
- [ ] Experiments and findings
  - [ ] Metrics
  - [ ] 0) Picking a Reward function
  - [ ] 1) Training only on NORMAL settings
  - [ ] 2) Training with a bit of ABNORMAL settings
  - [ ] 3) Picking an Observation function
  - [ ] 4) kickstarting a self-adapting system
  - [ ] -) Impact of cycle time in fixed cycle agents
  - [ ] -) Impact of resizing quantization of states
  - [ ] -) Impact of resizing neural networks
  - [ ] -) Impact of agent expertise levels during multi-agent system training
  - [ ] -) Transfer learning between agents of same shape (modularization and idempotence)
  - [ ] -) Transfer learning between agents of different shape (mutation and adaptation)
- [ ] Threats to Validity
  - [ ] Internal threats
    - [ ] Simplistic simulation model
    - [ ] No advanced tweaks in RL Models
  - [ ] External threats
    - [ ] Tiny scenarios
    - [ ] No experimentation on wide networks with OD Matrix
- [ ] Future Work
  - [ ] Apply results to Via Bassini to lower waiting time for ATM trolleybus 93, my beloved
  - [ ] Apply results to Via Porto Corsini to catch ATM M1 in time for Bisceglie instead of Rho Fieramilano
- [ ] Conclusion
