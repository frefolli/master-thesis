# Unconventional reinforcement learning on traffic lights with SUMO

## Thesis or Master Degree in Computer Science, academic year 2024/2025

## Index

- [ ] Introduction
- [x] Traffic Science
  - [x] Introduction
  - [x] Transit regulation
  - [x] Demand models
  - [x] Settings and assumptions
- [x] Traffic Simulation
  - [x] SUMO
  - [x] Multi agent transit simulations
  - [x] SUMO-RL
- [x] Background on Reinforcement Learning
  - [x] A new perspective
    - [x] Learning by example
    - [x] Learning by inference
    - [x] Learning by experience
  - [x] Mathematics foundation of Environment Modeling and Reinforcement Learning
    - [x] Markov Chains
    - [x] Markov Decision Processes
    - [x] Policies
    - [x] Value Functions
      - [x] State-value function
      - [x] Action-value function
      - [x] Usefulness of value functions
  - [x] Learning to learn
    - [x] Learning techniques
      - [x] Model-Based/Free learning
      - [x] On/Off-Policy learning
      - [x] Monte Carlo methods
      - [x] Temporal Difference methods
      - [x] Policy-gradient methods
      - [x] Actor-Critic methods
    - [x] Reinforcement Learning Algorithms
      - [x] SARSA
      - [x] Q Learning
      - [x] Double Q Learning
      - [x] DQN
      - [x] PPO
  - [x] Design considerations
    - [x] Environment design
      - [x] The state space
      - [x] The action space
      - [x] The observation function
      - [x] The reward function
    - [x] Experience design
      - [x] Incremental learning
      - [x] Curriculum learning
      - [x] Simulated learning
- [ ] Fasteners and tools
  - [ ] The Core
    - [ ] The Architecture
    - [ ] The Environment
    - [ ] The Observation Functions
      - [ ] ObservationFunction
      - [ ] DefaultObservationFunction
      - [ ] DensityObservationFunction
      - [ ] QueueObservationFunction
      - [ ] SpeedObservationFunction
      - [ ] PhaseObservationFunction
      - [ ] SharedVisionObservationFunction
    - [ ] The Reward Functions
      - [ ] RewardFunction
      - [ ] AverageSpeedRewardFunction
      - [ ] DiffWaitingTimeRewardFunction
      - [ ] PressureRewardFunction
      - [ ] QueueLengthRewardFunction
      - [ ] DiffQueueLengthRewardFunction
      - [ ] MixedRewardFunction
      - [ ] SharedVisionRewardFunction
    - [ ] The Agents
      - [ ] Agent
      - [ ] Learning Agents
        - [ ] SARSAAgent
        - [ ] QLAgent
        - [ ] DQLAgent
        - [ ] DQNAgent
        - [ ] PPOAgent
      - [ ] Fixed Cycle Agents
        - [ ] FixedCycleAgent
        - [ ] Fixed15
        - [ ] Fixed30
        - [ ] Fixed45
        - [ ] Fixed60
  - [ ] The Tools
    - [ ] Importing from external sources
      - [ ] amma2cityflow
      - [ ] amma2sumo
      - [ ] cityflow2sumo
    - [ ] Generating topologies
      - [ ] generate-topology
    - [ ] Generating traffic
      - [ ] flows
      - [ ] generate-flows
      - [ ] generate-datasets
    - [ ] Executing experiments
      - [ ] executor
    - [ ] Extracting metrics
      - [ ] extract-directional-metrics
      - [ ] extract-global-metrics
    - [ ] Plotting metrics
      - [ ] plot-global-metrics
      - [ ] plot-smoothed-metrics
      - [ ] plot-directional-metrics
    - [ ] Comparing metrics
      - [ ] merge-experiments
      - [ ] compare-global-metrics
      - [ ] compare-directional-metrics
    - [ ] Generating reports
      - [ ] generate-report
- [ ] Experiments and findings
  - [ ] Q0: Acquire a baseline
    - [ ] E0: Find Best Reward Function
    - [ ] E1: Find Best Observation Function
  - [ ] Q1: Curriculum vs Frankestein
    - [ ] E2: Find The Best Dataset
  - [ ] Q2: Find the best agent model
    - [ ] E3A: Find The Best Tabular Agent
    - [ ] E3B: Find The Best Deep Agent
    - [ ] E3C: Find The Best Fixed Agent
    - [ ] E7: Try Unattended
    - [ ] E8: Try Unquantized
  - [ ] Q3: Self-Adaptive
    - [ ] E4: Try Self Adaptive
  - [ ] Q4: Single Agent vs Multi Agent
    - [ ] E5: Try Marl On Observation
    - [ ] E6: Try Marl On Reward
    - [ ] E9: Try Partition
- [ ] Threats to Validity
  - [ ] Internal threats
    - [ ] Simplistic simulation model
    - [ ] No advanced tweaks in RL Models
  - [ ] External threats
    - [ ] Tiny scenarios
    - [ ] No experimentation on wide networks with OD Matrix
- [ ] Future Work
  - [ ] Apply results to Via Bassini to lower waiting time for ATM trolleybus 93, my beloved
  - [ ] Apply results to Via Porto Corsini to catch ATM M1 in time for Bisceglie instead of Rho Fieramilano
- [ ] Conclusion
