\section{Mathematics behind the scene}

Reinforcement Learning (RL) provides a framework for agents to learn optimal behavior through trial and error interactions with a dynamic environment. Unlike supervised learning, where models are trained on labeled input-output pairs, RL relies on feedback in the form of scalar rewards to guide behavior. The formal underpinnings of RL are grounded in the theory of \textit{Markov Decision Processes} (MDPs), which model sequential decision-making under uncertainty.

\subsection{Markov Decision Processes}

An MDP is typically defined as a 5-tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ is the set of all possible states, $\mathcal{A}$ is the set of available actions, $P(s'|s, a)$ denotes the transition probability of moving to state $s'$ when taking action $a$ in state $s$, $R(s, a)$ is the expected immediate reward received for taking action $a$ in state $s$, and $\gamma \in [0,1)$ is the discount factor that models the agent’s preference for immediate versus future rewards.

A fundamental assumption in MDPs is the \textit{Markov property}, which states that the future is conditionally independent of the past given the present state. Mathematically, this implies that:
\[
\mathbb{P}(s_{t+1} \mid s_0, a_0, \dots, s_t, a_t) = \mathbb{P}(s_{t+1} \mid s_t, a_t).
\]

The agent's behavior is governed by a \textit{policy}, denoted by $\pi$. A policy can be deterministic, mapping each state to a specific action $\pi(s) = a$, or stochastic, defined as a probability distribution over actions given states, $\pi(a|s)$.

\subsection{Value Functions and the Bellman Equations}

Central to reinforcement learning is the concept of \textit{value functions}, which quantify the expected long-term return of states or state-action pairs under a given policy. These functions play a crucial role in policy evaluation and improvement.

The \textit{state-value function}, $V^\pi(s)$, is defined as the expected return when starting in state $s$ and following policy $\pi$ thereafter:
\[
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s \right].
\]

Similarly, the \textit{action-value function}, $Q^\pi(s, a)$, represents the expected return starting from state $s$, taking action $a$, and subsequently following policy $\pi$:
\[
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s, a_0 = a \right].
\]

These value functions satisfy recursive relationships known as the \textit{Bellman expectation equations}. For the state-value function:
\[
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) V^\pi(s') \right],
\]
and for the action-value function:
\[
Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s', a').
\]

\subsection{Optimality and Bellman Optimality Equations}

The goal in reinforcement learning is to find an \textit{optimal policy} $\pi^*$ that maximizes the expected return from every state. The corresponding \textit{optimal value functions} are:
\[
V^*(s) = \max_\pi V^\pi(s), \quad Q^*(s, a) = \max_\pi Q^\pi(s, a).
\]

These optimal value functions satisfy the \textit{Bellman optimality equations}, which are defined as:
\[
V^*(s) = \max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) V^*(s') \right],
\]
\[
Q^*(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) \max_{a' \in \mathcal{A}} Q^*(s', a').
\]

Solving these equations—either exactly in small MDPs or approximately in large or continuous environments—is central to the development of reinforcement learning algorithms. Classical methods such as value iteration and policy iteration directly exploit these equations, while modern approaches like Q-learning and deep reinforcement learning extend them to more complex settings.

