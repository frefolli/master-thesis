\chapter{Fasteners and Tools}

This chapter showcases and explains the architecture of the extended reinforcement learning framework \textit{SUMO-RL}, along with its components, tools and algorithms.

\section{The Core}

This section is concerned with the core of the reinforcement learning framework, containing software for implementing agents, functions and other enabling components.

\subsection{The Architecture}

SUMO-RL has a \textbf{modular} architecture (it can be seen in figure \ref{fig:sumo-rl-architecture}) that allows developers and researchers to replace a-la-carte almost all its components if needed.

The \textit{Environment} object get created through settings supplied by both command line arguments and a configuration file.
These settings select which functions, agents, scenario to use in the simulation, as well as feature flags for tuning SUMO and changing other aspects of the experiments.
The Environment is then equipped with an \textit{ObservationFunction}, a \textit{RewardFunction}, a dataframe for collecting \textit{Metrics} about agents and the simulations, an object for caching data about vehicles, lanes and intersections called \textit{Datastore}, and a dictionary of \textit{TrafficSignal}s.
Each \textit{TrafficSignal} instance represents a controlled intersection in the scenario that has been loaded in the \textit{Environment}. Upon creation, it queries SUMO to extract valuable data about the shape of the intersection and the traffic light program logic (that is to say \textit{the clock cycle and its phases}). It identifies its \textit{green phases} and represents the act of switching to one of those as an action of the Action Space.

Along with the environment, also a set of \textit{Agent}s are created which can observe the state of the simulation through the output of the \textit{ObservationFunction} and get rewarded with the output of the \textit{RewardFunction}, both of which are mediated by the \textit{Environment}.

In most of the literature works either the is only one intersection in the scenario or each agent instance is controlled strictly one of the intersections. As such, also the original SUMO-RL framework \cite{sumorl} developed by Lucas Alegre supported an agent to control only one traffic light intersection.
The architecture has been revised to allow agents to control more than one intersection by modifying both the agent trait and how the environment works.
In particular, SUMO-RL has been equipped with an algorithm for partitioning the set of controllable traffic lights intersections on the basis of the number of input lanes or the State Space shape.
By default, it assigns an distinct instance of agent to each one of the intersections in the network. If needed, it will group intersections based on the aforementioned similarities and assign a single agent instance for each group.
Furthermore, while the architecture allows for different agent types to act in the simulation at the same time, the system is actually instructed to use only one agent type at a time, because the goals of this research don't require to analyze the behaviour of different agent types on the same network.

\putimage{figures/sumo-rl-architecture.png}{The SUMO-RL Architecture}{fig:sumo-rl-architecture}{1.0}

\subsection{The Environment}

%% TODO: inserire citazione a Gymnasium

\paragraph{Loading the scenario}

The Environment component, upon creation, starts a connection with SUMO loading the scenario that has been selected. Then, it queries SUMO for IDs of traffic lights and creates and instance of TrafficSignal for each one of them passing their ID and a copy of the connection with SUMO.
The TrafficSignal constructor also accepts parameters for changing the minimum green-light duration, the maximum green-light duration and the yellow-light duration.
This way, the TrafficSignal component is able to query SUMO for its shape and traffic light control program.
Therefore it acquires the list of incoming lanes, the list of output lanes and it caches the lengths of its lanes (which will be used afterwards in computations).
It also sets its \textit{Action Space} with a \textit{gymnasium}'s Discrete space which is parametrized on the number of green phases.
The action on a traffic light is the green phase it should enable, then its up to the TrafficSignal component to enforce that phases by using the correct yellow-light transition and the correct phase in the simulations.

\paragraph{Acquiring traffic light phases}

A TrafficSignal doesn't use the program hard-coded in the SUMO scenario but construct an appropriate program analyzing the hard-coded one.
It starts fetching all phases and filling an array \textit{green-phases} with all the phases which contain at least on of green-light encoding characters (\textit{"gG"}).
The green-phases are added also to the list of output \textit{phases}.
Then, it iterates on couples of sequential green phases to build the yellow phases which are a transition from a green phases $A$ and a green phase $B$. It essentially copies the phase of $A$ replacing all occurrences of green-light characters with the yellow-light character (\textit{"y"}).
It also keep a map \textit{yellow-dict} of transitions which can be indexed by a tuple of indices ($<i, j>$) and returns the corresponding yellow phase index.

\paragraph{Running the simulation}

Before running each simulation the environment should be reset through the \textit{reset()} method.
Every time it is reset, it reloads SUMO with the previously selected scenario, the selected routes file and the input seed (important for ensuring that a performed simulation is replicable).
At each iteration of a simulation, the agents are fed with the observations granted from the Environment and their actions are collected and fed in the Environment with its \textit{step()} method.
Afterwards, some other methods can be called depending of what data is needed. At least \textit{gather\_data\_from\_sumo()} and {compute\_observations()} are needed for running a simulation but using also \textit{compute\_rewards()} allows agents to learn and the researcher to investigate results.

\begin{itemize}
  \item the \textit{gather\_data\_from\_sumo()} method essentially queries SUMO for data concerning vehicles and lanes. For each vehicle the following metrics are extracted:
  \begin{itemize}
    \item \textit{awt}: Accumulated Waiting Time is the amount of time of the last $1000$ seconds that a vehicle has spent being at full-stop.
  \end{itemize}
  For each lane, the following data are extracted:
  \begin{itemize}
      \item \textit{lsvn}: LastStepVehicleNumber is the number of vehicles present in the lane in the last step of the simulation.
      \item \textit{lshn}: LastStepHaltingNumber is the number of full-stop vehicles in the lane in the last step of the simulation.
      \item \textit{lsms}: LastSteoMeanSpeed is the average speed of vehicles in the lane in the last step of the simulation.
      \item \textit{lso}: LastStepOccupancy is the occupancy level in the lane in the last step of the simulation.
      \item \textit{lswt}: LastStepWaitingTime is the average waiting time of vehicles in the lane in the last step of the simulation.
      \item \textit{vehs}: LastStepVehicleIDs is the list of vehicles present in the lane in the last step of the simulation.
      \item \textit{mawt}: the MeanAccumulatedWaitingTime is computed by averaging accumulated waiting times of vehicles in a lane.
      \item \textit{tawt}: the TotalAccumulatedWaitingTime is computed by summing accumulated waiting times of vehicles in a lane.
  \end{itemize}
  \item the \textit{compute\_observations()} method construct observations for each traffic light with the selected ObservationFunction and stores them inside an Environment field.
  \item the \textit{compute\_rewards()} method constructs the rewards using the selected RewardFunction and stores them inside an Environment field.
  \item the \textit{compute\_metrics()} method computes the following metrics:
  \begin{itemize}
    \item \textit{step}: the current simulation step
    \item \textit{total\_running}: the number of vehicles which speed is not zero.
    \item \textit{total\_backlogged}: the number of vehicles which are enqueued and waiting to enter the simulation.
    \item \textit{total\_stopped}: the number of vehicles at full-stop.
    \item \textit{total\_arrived}: the number of vehicles which have arrived at their destination in the last step.
    \item \textit{total\_departed}: the number of vehicles which have departed in the last step (this is the same as \textit{entering the simulation}).
    \item \textit{total\_waiting\_time}: the sum of waiting time of all lanes.
    \item \textit{mean\_waiting\_time}: the mean of waiting time of all lanes.
    \item \textit{total\_accumulated\_waiting\_time}: the sum of accumulated waiting time of all lanes.
    \item \textit{mean\_accumulated\_waiting\_time}: the mean of accumulated waiting time of all lanes.
    \item \textit{mean\_speed}: the average of speed of all lanes.
    \item \textit{total\_reward}: the total of rewards released by the environment in the last step.
  \end{itemize}
  These are written to CSV files and can be analyzed to compute the performance of models as well as evaluating their fairness with respect of directions. This last option is available by setting to True a parameter called \textit{advanced\_metrics} which enables tracking the aforementioned metrics for each direction of the network (each pair of origin-destination inside the network).
\end{itemize}

\subsection{The Observation Functions}

The ObservationFunction is an functor object which uses data from the \textit{Datastore} and the \textit{TrafficSignal}. It supports quantization of continuous values to a configurable number of fixed levels. This operation is performed in conformity with equation \ref{eq:quantization}. By default, the number of levels is $16$, but this value can be changed by command line as needed and if it's set to zero, then no quantization is performed (this is useful for testing models with continuous spaces).
The State Space is defined by a $gymnasyum$'s Box of floating point values in range $[0, 1]$ of length \textit{observation\_space\_size()}.

In Table \ref{tbl:observation-features} the features which can be extracted for composing the observation of an agent are listed and detailed.
Moreover, a \textit{shared-view} mechanism has been implemented so that a traffic light agent can see not only its state through a observation function (called "me-function") but also a view over its neighbour traffic lights through a second observation function (called "you-function").
The shared-view is implemented with a cache so that the observation of a traffic light isn't computed two or more times for each time step.
Each agent has a owned observation and if the shared-view is active, then it imports in the resulting observation also the owned observation of neighbour agents.

It was chosen not to include output lane information in the observable features in order to keep the state space small. Literature works on the same topic usually keep this convention and adopts similar state features \cite{wei2019presslight} \cite{han2023leveraging}. The novelty is the inclusion of neighbour data, which also makes up for the lack of output lane data since a output lane of a intersection A can be an input lane of a neighbour intersection B.

In Table \ref{tbl:observation-functions} the available configurations for observation functions are listed by what features they allow the agent to see and if they are \textit{shared-views}. The output State Space shape is also displayed assuming that a traffic light agent has $N$ phases, $M$ incoming lanes and $K$ neighbours.
The reason behind the choice of using the Density ("d") observation function as base for the shared-views will be more clear in the next chapter.

\begin{table}[H]
  \captionof{table}{Summary of implemented observation features}
  \label{tbl:observation-features}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|l|c|c|}
      \hline
      Name & ID & Description & Formula \\
      \hline
      Current Phase Encoding     & \textbf{CPE} & \makecell[l]{the current traffic light phase ($CP$) encoded as one-hot vector.} &
      \makecell{$
        CPE = \left<
          \left\{
          \begin{array}{cl}
            1 & CP = P_i \\
            0 & CP \ne P_i
          \end{array}
          \right\}
          | \; \forall i \in [0, N)
        \right>
      $} \\
      \hline
      Minumum Green Time Reached & \textbf{MGR} & \makecell[l]{a boolean value which is $1$ if and only if the time since \\
                                                               when the traffic light completed a phase change ($TSLC$) has surpassed \\
                                                               the environment configured minimum green time ($MGT$).} &
      \makecell{$
        MGR = \left\{
          \begin{array}{cl}
            1 & TSLC > MGT \\
            0 & TSLC \leq MGT
          \end{array}
        \right\}
      $} \\
      \hline
      Lane Speed Percentage      & \textbf{LSP} & \makecell[l]{for each lane the average speed ($S(v)$) of vehicles in it ($V(L_i)$) is \\
                                                               divided by the maximum allowed speed on such lane ($MAS(L_i)$).} &
      \makecell{$LSP = \left< \frac {\sum _ {v \in V(L_i)} S(v)} {|V(L_i)| \; \cdot \; MAS(L_i)} \; | \; \forall i \in [0, M) \right>$} \\
      \hline
      Lane Occupancy             & \textbf{LOC} & \makecell[l]{for each lane, the percentage of its occupancy \\
                                                               is retrieved from SUMO. It is equivalent to summing \\
                                                               the lengths ($|v|$) of vehicles in a lane ($V(L_i)$) \\
                                                               divided by the length of that lane ($|L_i|$)} &
      \makecell{$LOC = \left< \frac {\sum _ {v \in V(L_i)} |v|}  {|L_i|} \; | \; \forall i \in [0, M) \right>$} \\
      \hline
      Lane Queuing Percentage    & \textbf{LQP} & \makecell[l]{for each lane, its occupancy ($LOC_i$) is multiplied by \\
                                                               the number of vehicles in such lane being at full-stop ($FSV(L_i)$) \\
                                                               and divided by the number of vehicles in that lane ($V(L_i)$).} &
      \makecell{$LQP = \left< \frac{{LOC}_i \; \cdot \; |FSV(L_i)|} {|V(L_i)|} \; | \; \forall i \in [0, M) \right>$} \\
      \hline
    \end{tabular}
  }
\end{table}

\begin{table}[H]
  \captionof{table}{Summary of implemented observation functions}
  \label{tbl:observation-functions}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}
      \hline
      ID & \multicolumn{5}{|c|}{Itself} & \multicolumn{5}{|c|}{Neighborhood} & State Space Shape \\
      \hline
      & CPE & MGR & LSP & LOC & LQP & CPE & MGR & LSP & LOC & LQP & \\
      \hline
      default & \FilledCircle & \FilledCircle & \FilledCircle & \FilledCircle & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & ${[0,1]} ^ {log _ 2 {N} + 1 + 3 \cdot M}$ \\
      \hline
      s       & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & ${[0,1]} ^ {M}$ \\
      \hline
      d       & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & ${[0,1]} ^ {M}$ \\
      \hline
      q       & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & ${[0,1]} ^ {M}$ \\
      \hline
      sv      & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \FilledCircle & \FilledCircle & \FilledCircle & \FilledCircle & \FilledCircle & ${[0,1]} ^ {log _ 2 {N} + 1 + (3 + K) \cdot M}$ \\
      \hline
      svs     & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & ${[0,1]} ^ {(K + 1) \cdot M}$ \\
      \hline
      svp     & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & ${[0,1]} ^ {(K + 1) \cdot M}$ \\
      \hline
      svd     & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & ${[0,1]} ^ {(K + 1) \cdot M}$ \\
      \hline
      svq     & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & ${[0,1]} ^ {(K + 1) \cdot M}$ \\
      \hline
    \end{tabular}
  }
\end{table}

\subsection{The Reward Functions}

The RewardFunction is an functor object which uses data from the \textit{Datastore} and the \textit{TrafficSignal}.

In Table \ref{tbl:reward-features} the features which can be extracted for composing the reward of an agent are listed and detailed.
Moreover, a \textit{shared-view} mechanism has been implemented so that a traffic light agent can be rewarded also with its neighbour rewards.
The shared-view is implemented with a cache so that the reward of a traffic light isn't computed two or more times for each time step.
Each agent has a owned reward and if the shared-view is active, then it imports in the resulting reward also the owned reward of neighbour agents.

The DWT measures how much the accumulated waiting time has decreased. If $DWT > 0$ then the accumulated waiting time is lower than before and it delivers a positive reward.

The SPD measures if vehicles are flowing at a higher speed than half the maximum allowed speed. Intuitively, if this quantity is positive, then it leads to a positive reward.

The QLE without negative sign would be non-negative number. Since its measuring the amount of halted vehicles, a negative reward is released when the absolute value of this quantity is greater then 0. This way, the agents will minimize the length of queues in controlled lanes.
Moreover, since DQL is using QLE are reference, it doesn't need to be inverted.

Finally, PRE is based on the concept of Pressure \cite{wei2019presslight}, which expresses the disequilibrium between the outcoming flow and the incoming flow.
The original formula, shown in equation \ref{eq:original-pressure}, was the difference of incoming and outcoming vehicles ($V(L_i)$) weighted by the capacity of lanes ($LC(L_i)$), which in PRE has been simplified by removing the divisions and inverting the involved quantities to deliver a positive reward whenever the outcoming flow surpasses the incoming flow.
Intuitively this means that the number of vehicles which have passed the intersection is higher than the one of queued and incoming vehicles.
More will be said in the following sections about how the capacity of a road can be calculated, but in the present research I'm assuming that all lanes have similar capacity due to the fact that the context is urban tissue which usually have $30-50$ km/h speed limits on the whole road network.

In Table \ref{tbl:reward-functions} the available configurations for reward functions are listed by what features are used and if they are \textit{shared-views}. The output Reward Space shape is also displayed assuming that a traffic light agent has $N$ incoming lanes, $M$ outcoming lanes and $K$ neighbours.
The reason behind the choice of using the DiffWaitingTime ("dwt") reward function as base for the shared-views will be more clear in the next chapter.

\begin{table}[H]
  \captionof{table}{Summary of implemented reward features}
  \label{tbl:reward-features}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|l|c|c|}
      \hline
      Name & ID & Description & Formula \\
      \hline
      Diff Accumulated Waiting Time  & \textbf{DWT} & \makecell[l]{the difference between the total accumulated waiting time ($TAWT$) \\
                                                                   of vehicles in its input lanes in the previous time step and the TAWT \\
                                                                   in the current time step. The DWT is divided by 100 to avoid unstable \\
                                                                   rewards due to the TAWT having an upper bound of $1000$.}
                                                    & \makecell{$ \frac {{TAWT}_{t-1} - {TAWT}_{t}} {100} $} \\
      \hline
      Average Speed                  & \textbf{SPD} & \makecell[l]{the mean of average speeds of vehicles in its input lanes $AS(L_i)$ \\
                                                                   divided by the maximum allowed speed ($MAS(L_i)$) minus $\frac {1} {2}$.}
                                                    & \makecell{$\sum _ {i \in [0, N)} \frac {AS(L_i)} {MAX(L_i)} - \frac {1} {2}$} \\
      \hline
      Queue Lengths                  & \textbf{QLE} & \makecell[l]{the mean of length of queues (as number of full-stop vehicles $FSV(L_i)$) \\
                                                                   in its input lanes multiplied by $-1$.}
                                                    & \makecell{$- \frac {\sum _ {i \in [0, N)} FSV(L_i)} {N}$} \\
      \hline
      Diff Queue Lengths             & \textbf{DQL} & \makecell[l]{the difference between the QLE in the current time step and \\
                                                                   the QLE in the previous \\ time step.} & \makecell{${QLE}_{t} - {QLE}_{t-1}$} \\
      \hline
      Pressure                       & \textbf{PRE} & \makecell[l]{for each lane, the negative pressure is defined as the difference between \\
                                                                   the number of outcoming vehicles and the number of incoming vehicles}
                                                                   & \makecell{$P = \left[ \sum _ {i \in [0, N)} V(L_i) \right] - \left[ \sum _ {i \in [0, M)} V(L_i) \right]$} \\
      \hline
    \end{tabular}
  }
\end{table}

\begin{equation} \label{eq:original-pressure}
  P =
  \left[ \sum _ {i \in [0, N)} \frac {V(L_i)} {LC(L_i)} \right]
  -
  \left[ \sum _ {i \in [0, M)} \frac {V(L_i)} {LC(L_i)} \right]
\end{equation}

\begin{table}[H]
  \captionof{table}{Summary of implemented reward functions}
  \label{tbl:reward-functions}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
      \hline
      ID & \multicolumn{5}{|c|}{Itself} & \multicolumn{5}{|c|}{Neighborhood} \\
      \hline
      & DWT & SPD & QLE & DQL & PRE & DWT & SPD & QLE & DQL & PRE \\
      \hline
      dwt   & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  \\
      \hline
      as    & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  \\
      \hline
      ql    & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  \\
      \hline
      dql   & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  \\
      \hline
      p     & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  \\
      \hline
      svdwt & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  \\
      \hline
      svas  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  \\
      \hline
      svql  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  \\
      \hline
      svdql & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  \\
      \hline
      svp   & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \EmptyCircle  & \FilledCircle \\
      \hline
    \end{tabular}
  }
\end{table}

\subsection{The Agents}

The agents which have been implemented are listed in Table \ref{tbl:agents}.
The agents \textit{fixed15}, \textit{fixed30}, \textit{fixed45}, \textit{fixed60} follow a fixed cycle algorithm, switching phase every $k$ seconds.
Those are the baseline for comparing RL algorithms with the currently employed system. Since all agents start synchronized in the same phase, they are operated in \textit{green wave} mode.

\begin{table}[H]
  \captionof{table}{Summary of implemented agents}
  \label{tbl:agents}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|c|c|c|c|c|c|}
      \hline
      ID      & Agent Type  & Cycle Time & Learning Algorithm           & Learning Method & Learning Policy                & Notes \\
      \hline
      fixed15 & Fixed Cycle & 15 secs    & \xmark                       & \xmark          & \xmark                         &       \\
      \hline
      fixed30 & Fixed Cycle & 30 secs    & \xmark                       & \xmark          & \xmark                         &       \\
      \hline
      fixed45 & Fixed Cycle & 45 secs    & \xmark                       & \xmark          & \xmark                         &       \\
      \hline
      fixed60 & Fixed Cycle & 60 secs    & \xmark                       & \xmark          & \xmark                         &       \\
      \hline
      sarsa   & RL Agent    & \xmark     & SARSA                        & Tabular         & On-Policy                      &       \\
      \hline
      ql      & RL Agent    & \xmark     & Q Learning                   & Tabular         & Off-Policy ($\epsilon$-Greedy) &       \\
      \hline
      dql     & RL Agent    & \xmark     & Double Q Learning            & Tabular         & Off-Policy ($\epsilon$-Greedy) &       \\
      \hline
      dqn     & RL Agent    & \xmark     & Deep Q Networks              & Neural          & Off-Policy (Buffer-based)      &       \\
      \hline
      ppo     & RL Agent    & \xmark     & Proximal Policy Optimization & Neural          & Off-Policy (Buffer-based)      &       \\
      \hline
    \end{tabular}
  }
\end{table}

\section{The Tools}

\subsection{Importing from external sources}

\paragraph{amma2cityflow}
\paragraph{amma2sumo}
\paragraph{cityflow2sumo}

\subsection{Generating topologies}

\paragraph{generate-topology}

\subsection{Generating traffic}

\paragraph{flows}
\paragraph{generate-flows}
\paragraph{generate-datasets}

\subsection{Executing experiments}

\paragraph{executor}

\subsection{Extracting metrics}

\paragraph{extract-directional-metrics}
\paragraph{extract-global-metrics}

\subsection{Plotting metrics}

\paragraph{plot-global-metrics}
\paragraph{plot-smoothed-metrics}
\paragraph{plot-directional-metrics}

\subsection{Comparing metrics}

\paragraph{merge-experiments}

\paragraph{compare-global-metrics}
\paragraph{compare-directional-metrics}

\subsection{Generating reports}

\paragraph{generate-report}
