\chapter{Background on Reinforcement Learning}

This chapter introduces the concept of Reinforcement Learning, along with its differences from traditional machine learning approaches, its advantages and disadvantages, and its mathematical foundation.

\section{A new perspective}

Machine Learning (ML) is a branch of Artificial Intelligence concerning all those statistical techniques and algorithms that can be used to learn a task from an existing data and generalize this knowledge to unseen data. Depending on how the agents learns and what task it learns, ML can be divided into three paradigms: supervised, unsupervised and reinforcement learning.

\paragraph{Learning by example}

Supervised learning (SL) is perhaps the most intuitive and widely used learning paradigm.
The agent is given an input and returns an output. Its output is then compared with the expected output (provided along with the input) using a \textit{Loss Function}, which measures the error between the two values.
This loss is then used to update the agent's model in order to improve its accuracy.
Since the quality of the agent is defined by the similitude between its output and the reference (``correct'') output, it is essentially learning to replicate a behaviour of which the labelled dataset is an example.

Despite its simplicity, this paradigm take two strong assumptions.
1) The current output of the agent doesn't influence the goodness of a future output. That is to say: each decision or value emitted from the agent is independent temporally from other values emitted over time.
Tasks involving classification or labelling, such as identifying the class of a dry bean or spam detection, fall naturally into this category.
2) A dataset of input-output pairs is available and, more importantly, a correct output value exists and it is priorly known or computable. This restriction automatically excludes all problems involving uncertainty, long-term consequences, or scenarios in which the optimal decision depends on a sequence of past and future actions. However, it accommodates a lot of problems for which knowledge about past is available, as in prediction tasks like stock forecasting and language models.

\paragraph{Learning by inference}

Unsupervised learning (US) removes the need of labelled data, as the goal here is not to imitate known outcomes but to find hidden structure within the data itself.
Usually, this activity involves partitioning the input dataset into k classes, which result can be then applied to further processing.
Although the ultimate goal is to ensure that items within the same group resemble each other more than they resemble those in other groups, different approaches to achieve such partitioning exist, ranging from distance-based algorithms (such as K-Means) to hashing-based algorithms (such as Locality Sensitive Hashing).
In both cases, the agent is inferring an hidden connection between elements of the dataset using its features.
However, it should be noted that the resulting classification carries no semantic beyond element similarity and, in particular, there is no concept as intrinsic partitioning correctness. Which means that US cannot be applied directly in decision making problems and can only be used for aggregation tasks such as ancestral reconstruction (useful in phylogenetics) and feature extraction.

\paragraph{Learning by experience}

A useful analogy comes from early human development. Consider a toddler learning to walk. The parent may encourage the child by standing across the room and using gestures or facial expressions (such as smiles or glares) to signal success or failure. The toddler does not receive explicit instructions or labelled examples of correct foot placement. Instead, learning unfolds through trial and error, with the child gradually discovering which patterns of movement lead to praise or comfort. Reinforcement learning (RL) mirrors this process: knowledge is acquired not through imitation or discovery of structure, but through experience and consequence.

The agent is acknowledged to lie into an environment, which has an internal logic and an internal state but those are not exposed to the agent.
The agent can only observe the environment with an \textit{Observation Function}, which returns a partial view (``observation'') of its state.
This observation is then used by the agent's decision-making process (``policy'') to take an action.
The environment updates its state with a \textit{Transition Function} which applies the effects of the agent's action to the current state and computes the next state.
The agent then collects the new observation and the reward released by the \textit{Reward Function} in order to improve its action.

Therefore, instead of learning from a static dataset, the agent learns by the experience gained when interacting with the environment.
Another key intuition is that the focus of RL is not on the single action, but on the outcome of a sequence of interactions. Thus, the agents aims to maximize the cumulative reward.
For that reason, RL is applicable in all those tasks in which the quality of an action is defined also by past and future actions, such as traffic light management.

\section{Mathematical foundation of Environment Modeling and Reinforcement Learning}

This section presents the theoretical concepts useful for understanding Reinforcement Learning models and algorithms.

\subsection{Markov Chains}

The evolution of RL environment could be roughly described as a \textbf{Markov Chain}, which is is a tuple $MC = (S, P)$ where $S$ is the set of all states and $P$ is a transition function such that $P(S_n | S_{n-1})$ denotes the probability of the state $S_n$ happening given that the previous state is $S_{n-1}$. An example of Markov chain is shown in figure \ref{fig:markov-chain}. Intuitively, any episode of an environment is a sequence of states and there is some kind of transition function which moves the environment from one state to another.

Notably, an important property of Markov Chains is that \textit{past and future are mutually independent}. This may sound like a contradiction to the concept of Markov Chain, but it is in fact a simplification over statistics which allows to model an environment more easily. The appearance of the current state is assumed to be influenced only by the appearance of the previous state. This principle can be summarized as $P(S_n | S_{n-1}) = P(S_n | S_{n-1}, S_{n-2}, S_{n-3}, S_{n-4})$.

Markov chains are a powerful tool for modelling stochastic environments, such as weather conditions. Supposing to have a dataset of daily weather (sunny, cloudy, rain, thunderstorm), one could compute an approximation of $P(S_n | S_{n-1})$ and use it for weather forecasting.

Now, recalling the Markov property and seeing this implementation, a valid argument against this practise of course could be that one precedent weather record isn't enough for predicting future records with sufficient accuracy. But states can also be aggregate or complex objects: Markov chains allow defining a state as the previous k records of weather data.

A more subtle detail resides in what it actually models: a Markov chain doesn't take into account the \textit{external forces} (agents' actions). For example, whether or not the Government decides to start \textit{cloud seeding} is already a game changer variable which radically changes the weather model.

\putimage{figures/markov-chain.png}{An example of three state Markov Chain}{fig:markov-chain}{0.5}

\subsection{Markov Decision Processes}

A \textbf{Markov Decision Process} is defined a tuple $MDP = (S, A, P, R)$ where S is the set of all states, $A$ is the set of all actions, $P$ is a transition function such that $P(S_n | S_{n-1}, a)$ denotes the probability of the state $S_n$ happening given that the previous state is $S_{n-1}$ with the action $a$, and $R$ is a reward function such that $R(S_{n-1}, a, S_{n})$ denotes the immediate reward of the transition from state $S_{n-1}$ to state $S_n$ with the action $a$. An example of MDP is shown in figure \ref{fig:markov-decision-process}.

This model not only accounts for agents' actions, but also provides an abstraction for expressing the quality of an action or the desirability of a certain state transition.

\putimage{figures/markov-decision-process.png}{The expansion of the Markov Chain of figure \ref{fig:markov-chain} as a Markov Decision Process. Green transitions release a positive reward, red transitions release a negative one.}{fig:markov-decision-process}{0.5}

\subsection{Policies}

The agent behaviour in a MDP environment can be defined with a \textbf{policy} $\pi$, which is a deterministic or stochastic mapping from the state set to the action set.
A deterministic policy $\pi_{det}$ is a function $\pi_{det} \; : \; S \rightarrow A$, while a stochastic policy $\pi_{sto}$ is a function $\pi _ {S} \; : \; S \times A \rightarrow [0, 1]$, denoted as $\pi _ {S}(a | s)$, where $\sum_{a \in A} \pi _ {sto} (s, a) = 1$.

The behaviour of an agent can then be evaluated by evaluating its policy: a policy $\pi _ a$ is \textit{better} than a policy $\pi _ b$ if the accumulated reward over time of $\pi _ a$ is higher than the one of $\pi _ b$.
In particular, a policy $\pi _ o$ is defined \textit{optimal} if $\forall \pi _ i \neq \pi _ o$, it holds that $\pi _o $ is \textit{better} than $\pi _ i$.
If the relative quality of a policy if defined by a scalar value, how can the accumulated rewards be computed?

\subsection{Value Functions}

\paragraph{State-value function}
\label{par:state-value-function}

The \textbf{state-value} function $V^{\pi}(s)$ evaluates the amount of expected reward obtained by following a policy $\pi$ from the state $s$.
Since both the environment and the agent's policy can be stochastic, an abstract formula to compute this concept reuses the expected value operator $\mathbb{E}$ from probability theory.
An slice of the episode of an MDP can be represented as a process graph starting from a node $s_i$, developing into a chain of states and actions. The value of this computation is given by a weighted sum of the rewards of tis transitions.
Since the same node and action can lead to different nodes, the process becomes a sequential, ramified structure as all possible transitions are recursively considered.
The expected reward of the state $s_i$ can be thought as the weighted sum of rewards of the average path along this ramified process, which leads to the following formula:

\begin{equation} \label{eq:v-pi-E}
V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t + 1}) \mid s_0 = s \right]
\end{equation}

Here, \textbf{$\gamma$} is discount factor used for weighting the sum of gained rewards. If $0 < \gamma < 1$, then $lim ^ {t \rightarrow +\infty} \gamma ^ t = 0$ and the immediate rewards have priority over long-term rewards. Otherwise, if $\gamma > 1$, then $lim ^ {t \rightarrow +\infty} \gamma ^ t = +\infty$ and the long-term rewards have priority over immediate rewards. The value of $\gamma$ should reflect the desirability of state transitions and the end goal of the agent learning in this environment. For this reason, it may also be the case for $\gamma = 1$ if immediate rewards and long-term rewards have the same importance in defining the goodness of the agent behaviour. For practical reasons, the abstract $V^{\pi}(s)$ formula can be rewritten to a more manageable recursive form:

\begin{equation} \label{eq:v-pi-bellman}
V^{\pi}(s) = \sum _ {a \in A} \pi (a | s) \sum _ {s' \in S} P(s' | s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right]
\end{equation}

\paragraph{Action-value function}
\label{par:action-value-function}

The \textbf{action-value} function $Q^{\pi}(s, a)$ evaluates the amount of expected reward obtained by following a policy $\pi$ from the state $s$ using the action $a$.
Reusing the reasoning of Subsection \ref{par:state-value-function}, the abstract formula can be written as:

\begin{equation} \label{eq:Q-pi-E}
Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1}) \mid s_0 = s, a_0 = a \right]
\end{equation}

Again, for practical reasons, the abstract $Q^{\pi}(s, a)$ formula can be rewritten to a more manageable recursive form:

\begin{equation} \label{eq:Q-pi-bellman}
Q^{\pi}(s, a) = \sum _ {s' \in S} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum _ {a' \in A} \pi (a' | s') Q^{\pi}(s', a') \right]
\end{equation}

\paragraph{Advantage function}

The \textbf{advantage} function $A^\pi(s, a)$ evaluates the potential advantage of choosing an action $a$ in terms of reward with respect to the average action reward from the state $s$. This is equivalent to computing $A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$.

\paragraph{Usefulness of value functions}

Both the \textit{state-value} function $V^{\pi}(s)$ \ref{par:state-value-function} and the \textit{action-value} function $Q^{\pi}(s, a)$ \ref{par:action-value-function} allow to compare different policies in order to find the most advantageous one at a certain point of time. Moreover, putting $s = s_0$ also allows to obtain an overall picture of quality of a policy. However, the function $Q^{\pi}(s, a)$ is also useful when constructing a winning policy. If a good approximation of $Q^{\pi}(s, a)$ is known, then the agent can choose the best performing action $a_o \in A$ using a simple comparison: $\forall a \in A \; | \; Q^{\pi}(s, a) \leq Q^{\pi}(s, a_o)$.

\section{Learning to learn}

This section introduces reinforcement learning patterns and principles for designing a good RL algorithm.

\paragraph{Model-Based/Free learning}

A \textit{model} is a representation of the internal logic of the Markov Decision Process. This model can also be a learned approximation. In fact, all algorithms in which the agent learns this representation are called \textbf{model-based}. Conversely, a \textbf{model-free} algorithm doesn't estimate the transition probabilities and reward function of the MDP. Instead, it directly learns an approximated optimal policy through trial and error, requiring no prior knowledge on the environment other than the shapes of the state and action sets.
In the present research, the focus is on \textit{model-free} learning agents, as the used RL algorithms will be explained in further sections.

\paragraph{On/Off-Policy learning}

As the model-free agent should gain experience interacting with environment, a question arise? Which policy should it use? A \textbf{on-policy} algorithm uses the currently known best policy which is being updated at every interaction. A \textbf{off-policy} algorithm uses the consolidated optimal policy to interact with the environment and gain experience (tuples state-action-reward, for example) which will be used at a later time to update the consolidated policy, building the new optimal policy. It is also possible to use a \textbf{on-policy} strategy combined with a greedy exploration of the state-action cross space.

For example, the \textbf{Epsilon-Greedy} (EG) \cite{liu2021improving} balances exploration (finding variants of a policy) and exploitation (using the optimal policy) phases using an decaying $\epsilon$ coefficient.
Every time an inference is needed, the EG algorithm peeks a random decimal number $x \in [0, 1]$. If $x \leq \epsilon$ then is chooses randomly an action to perform, otherwise it selects the current best action given by the policy.
Over time the $\epsilon$ coefficient lowers using a decaying factor $d$ so that $\epsilon_{t + 1} = \epsilon_{t} d$. This simple but effective algorithm allows to \textit{on-policy} algorithms (and also \textit{off-policy} ones) to explore the state-action space, since they otherwise risk of getting stuck to local optimums.
An important notice is that the EG algorithm is not by any degree a reinforcement learning algorithm by itself, it only enhance other RL algorithms.

\paragraph{Monte Carlo methods}

A \textbf{Monte Carlo} (MC) \cite{NIPS2014_88bf0c64} method exploits random exploration and averages the complete rewards obtained by those paths to update the approximation of the value functions.
As an example consider a Chess board in a given configuration $s_t$ at a time $t$. From this state there are multiple possible actions. A MC approach explores each one (as shown in Figure \ref{fig:monte-carlo-tree-chess}) with $k$ complete random walks and then computes averages in order to updates its action/state-value function approximation.

The Monte Carlo method is historically an impactful evolution of exhaustive recursive search which used a similar approach for exploring the state-action space but not randomness-driven. As can be easily imagined, many ``games'' such as Chess have a gazillion of possible states, and in general if the number of possible actions at each state is k, then number of possible state-action sequences after n moves is $\Omega(k^n)$. It should be noted that in case of stochastic environment, the number of possible paths is likely much higher. Using random walks and averaging results, MC manages to dominate the \textit{curse of dimensionality} while maintaining a decent accuracy.

Usually updates happen on a episode-by-episode basis, using only the cumulative reward for averages. Thus, MC doesn't account for distance in time of immediate rewards and in general it doesn't case of short-term rewards. As a consequence, MC is unable to update the value function for intermediate states.

\putimage{figures/monte-carlo-tree-chess.png}{An example of Monte Carlo random sampling for Chess}{fig:monte-carlo-tree-chess}{0.5}

\paragraph{Temporal Difference methods}

While Monte Carlo methods update value function estimates at the end of the episode, a \textit{temporal difference} (TD) \cite{sutton1988learning} method achieve the same results with incremental step-by-step updates.
In practise that means computing the next estimate for the action-value function $Q^\pi(s, a)$ (or the state-value function $V^\pi(s)$) with a weighted sum of the current estimate and of the immediate rewards.

For example, the tabular $TD(0)$ algorithm produces an approximation of the state-value function $V^\pi(s)$ using a simple update formula:

\begin{equation} \label{eq:TD0-update}
  {V_+}^\pi(s) = (1 - \alpha) V^\pi(s) + \alpha \left[ R(s, a, s') + \gamma V^\pi(s') \right]
\end{equation}

A clearer picture of what is happening behind the curtains can be seen by rewriting the equation:

\begin{equation} \label{eq:TD0-update-rewritten}
  {V_+}^\pi(s) = V^\pi(s) + \alpha \left[ \underbrace{R(s, a, s') + \gamma V^\pi(s')}_{hint} - \underbrace{V^\pi(S)}_{estimate} \right]
\end{equation}

A reader who is familiar with supervised learning techniques is probably recognizing a similarity with the general function approximation formula: $x \leftarrow x + \alpha \nabla f(x)$.
He would be right. TD methods use immediate rewards and current estimates of value functions to get a hint for approximating better those value functions.
That said, the precise nature of the hint may be speculated by specific RL algorithms. Approximating the state-value function $V\pi(s)$ is more trivial than updating the action-value $Q^\pi(s, a)$ function, which is however more effective for learning a policy. In the following section, some model-free TD algorithms are presented as they are used in the present research.

\paragraph{Policy-Gradient methods}

\textbf{Policy-Gradient} (PG) \cite{sutton1999policy} methods offer a direct way to learn policies without needing to estimate value functions first. These methods represent the policy as a parameterized function $\pi_\theta(a \mid s)$, where $\theta$ are the parameters (typically the weights of a neural network). The goal is to adjust these parameters to increase the expected reward the agent receives over time.

Instead of learning how good each state or action is, policy-gradient methods aim to increase the probability of actions that lead to better outcomes. This is done using the gradient of the expected return with respect to the policy parameters, known as the policy gradient. The agent collects experience, estimates how good the taken actions were, and updates the policy in the direction that would make these actions more likely in the future.

Using a MC-like exploration, it uses the current policy to draft N paths and then use them to infer the gradient descent.
The following is the equation for computing the gradient ($\nabla_\theta J(\theta)$) of the target function $J(\theta)$ with REINFORCE \cite{Sewak2019}, one of the first GB algorithms.

\begin{equation}
  \label{eq:policy-gradient-equation}
  \nabla _ {\theta} J(\theta) = \mathbb{E}
    \left[
      \sum _ {t \in [0, T]} \overbrace{\nabla _ {\theta} log \pi _ \theta (s_t, a_t)}^{\Delta} \overbrace{\sum _ {\tau \in [t, T]} (\gamma ^ {\tau - t} R _ \tau)}^{\mathbb{G}}
    \right]
\end{equation}

The expected value operator $\mathbb {E}$ averages the sum of an infinite amount of the gradient computed each on a single agent trail (driven by the policy or by randomness), therefore it can be approximated to a mean of a finite number of trails when N is \textit{big} (arbitrary, but a batch of $~1000$ samples is experimentally sufficient). In the equation two components are also highlighted. The gradient of log probabilities $\mathbb{\Delta}$ is the gradient of $log \pi _ \theta (s, a)$, which is $\Delta = \frac {\nabla _ \theta \pi _ \theta (s, a)} {\pi _ \theta (s, a)}$.

The function under the gradient operator $\nabla _ \theta$ is $\pi _ \theta (s, a)$, which is the distribution of probabilities (the stochastic policy) and its gradient is the direction of increment for the probability of the action $a$ in state $s$. Since $\pi _ \theta (s,a)$ is usually a neural network, its gradient can be seen as the gradient for expecting a one-hot vector as output with all zeros expect with the $1$ in correspondence of the action $a$.

Moreover, the gain quantity $\mathbb{G}$ is the estimated gain of the action $a$, which can be computed with full Monte Carlo interactions or can be learned (see Actor-Critic methods). Thus, the equation is essentially performing a weighted sum of gradients with respect of the expected gain of following the respective action.

Finally, the gradient is applied as usual with gradient descent methods. In the following formula uses the aforementioned gradient $\alpha \nabla _ {\theta} J(\theta)$ and a learning rate $\alpha$ to update the current policy.

\begin{equation}
  \theta \leftarrow \theta + \alpha \nabla _ {\theta} J(\theta)
\end{equation}

\paragraph{Actor Critic methods}

\textbf{Actor-Critic} (AC) \cite{grondman2012survey} methods combine the strengths of policy-gradient and value-based approaches. They consist of two interacting components:
\begin{itemize}
  \item The \textbf{actor}, which represents the policy $\pi_\theta(a \mid s)$ and decides which action to take.
  \item The \textbf{critic}, which estimates a value function (such as $V(s)$ or $Q(s, a)$) to evaluate how good the actor's actions are.
\end{itemize}

The critic provides feedback to the actor (essentially telling it how good its decisions were) so that the actor can update the policy more effectively, as shown in Figure \ref{fig:actor-critic-diagram}. This setup reduces variance in learning and leads to more stable updates compared to basic policy-gradient methods. In practice, the critic usually estimates either the value function or the advantage function. The actor then uses this information to adjust the policy parameters in a direction that improves performance. Basically, Equation \ref{eq:policy-gradient-equation} can be modified to replace the $\mathbb{G}$ component with the value function (such as the action-value function $Q^\pi(s, a)$) approximation learned by the critic:

\begin{equation}
  \label{eq:actor-critic-equation}
  \nabla _ {\theta} J(\theta) = \mathbb{E}
    \left[
      \sum _ {t \in [0, T]} \overbrace{\nabla _ {\theta} log \pi _ \theta (s_t, a_t)}^{\Delta} \overbrace{Q^\pi(s_t, a_t)}^{\mathbb{G}}
    \right]
\end{equation}

\putimage{figures/actor-critic-diagram.png}{The workflow of an Actor-Critic RL system. Source: ``Actor-critic reinforcement learning leads decision-making in energy systems optimization—steam injection optimization'', Abdalla et al, Springer \cite{abdalla2023actor}.}{fig:actor-critic-diagram}{0.5}

\section{Reinforcement Learning Algorithms}

In this section are covered the main RL algorithms that will be used in the experiments.

\subsection{SARSA}

\textbf{SARSA} (State-Action-Reward-State-Action), previously known as \textit{Modified Connectionist Q-Learning} (MCQ-L) \cite{rummery1994line}, is an on-policy Temporal Difference (TD) algorithm that updates its action-value function based on the action actually taken by the current policy. The update rule is defined as:

\[
  Q^\pi(s, a) \leftarrow Q^\pi(s, a) + \alpha \left[ R(s, a, s') + \gamma Q^\pi(s', a') - Q^\pi(s, a) \right]
\]

Here, $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$ is a complete SARSA quintuple. The algorithm’s on-policy nature means that the action $a_{t+1}$ is drawn from the current behavior policy, typically $\epsilon$-greedy. SARSA converges under standard stochastic approximation conditions if all state-action pairs are visited infinitely often.
The values of $Q^\pi(s, a)$ are initialized with very low values instead of random values or zeros for better convergence.

It is very easy to implement and is effective for low dimensional state spaces, but its convergence is slow and scales badly over big state spaces.
Also, it is important to notice that a state presented to the agent which he has not seen before are a \textit{miss} even if it already saw \textit{similar} states.
Moreover, SARSA (as other TD methods) lacks support for continuous action/state spaces.

\subsection{Q-Learning}

\textbf{Q-Learning} (QL) \cite{watkins1992q} is an off-policy TD method that seeks to learn the optimal policy independently of the agent’s actions. The core update rule is:

\[
  Q^\pi(s, a) \leftarrow Q^\pi(s, a) + \alpha \left[ R(s, a, s') + \gamma \max_{a'} Q^\pi(s', a') - Q^\pi(s, a) \right]
\]

The key difference from SARSA is the use of the greedy action in the update, regardless of the action actually taken. This makes Q-Learning an off-policy method, capable of learning optimal behavior while exploring with a different policy. It has strong convergence guarantees under suitable learning rate decay and sufficient exploration.
However, it suffers the same problems as SARSA due to its tabular nature, even if Q-learning does converge faster than SARSA and it allows to initialize simply to zeros the values of $Q^\pi(s, a)$.

As a side note, it is important to remember that tabular methods have the advantage to be translatable to hardware-driven solutions when an agent has learned enough. For example, a QL agent in a \textbf{low dimensional state space} can be translated to a digital circuit for better efficiency and scalability. However, since the environment can change over time (e.g. car drivers habits can change over time), those circuits may need to be modified if the agent's policy changes drastically over time. This can be solved with technologies such as FPGA which, at the cost of hardware complexity, allows change the circuit design when needed on every board.

\subsection{Double Q-Learning}

\textbf{Double Q-Learning} (DQL) \cite{hasselt2010double} is a variant of \textit{Q-Learning} which approximate two separate and mutually learning value functions: $Q^{\pi A}$ and $Q^{\pi B}$. The reason behind this choice is that approximating the future maximum reward with the same Q function that is being learned, in noisy environments, can lead to overestimates of action values, slowing convergence and worsening the quality of the learned policy.

To solve this problem, DQL learns two separate functions, which update works as usual except from the fact that the future maximum reward is driven by the other function. In practice, this means that in order to update $Q^{\pi A}$, the maximum future reward is computed with $Q^{\pi B}$ on the ending state $s'$ with the action $a'$ taken greedily from $Q^{\pi A}$. The update phase will then look like Equations \ref{eq:dql-update-formula}.

\begin{equation}
  \label{eq:dql-update-formula}
  \begin{split}
    Q^{\pi A}(s, a) & \leftarrow Q^{\pi A}(s, a) + \alpha \left[ R(s, a, s') + \gamma Q^{\pi B}(s', \max_{a'} Q^{\pi A}(s', a')) - Q^{\pi A}(s, a) \right] \\
    Q^{\pi B}(s, a) & \leftarrow Q^{\pi B}(s, a) + \alpha \left[ R(s, a, s') + \gamma Q^{\pi A}(s', \max_{a'} Q^{\pi B}(s', a')) - Q^{\pi B}(s, a) \right]
  \end{split}
\end{equation}

\subsection{DQN}

\textbf{DQN} (Deep Q-Networks) \cite{osband2016deep} extends Q-Learning to high-dimensional state spaces by using deep neural networks to approximate the $Q$-function.
DQN also incorporates a separate network used to compute stable maximum future reward values. This second network is called \textit{target network} and it is updated periodically to stabilize learning.
The update pass then becomes:

\[
  Q^{\theta}(s, a) \leftarrow Q^{\theta}(s, a) + \alpha \left[ R(s, a, s') + \gamma \max_{a'} Q^{\theta ^ -}(s', a') - Q^{\theta}(s, a) \right]
\]

Here, $\theta$ are the parameters of the online network and $\theta^{-}$ are the parameters of a target network.
These innovations address the instability of training neural networks with non independent-and-identically-distributed (\textit{IID}) data and bootstrapped targets.
DQN performs well in complex environments such as Atari games, though it can struggle with continuous action spaces or long-term credit assignment.
However, it must be regarded that all algorithms employing neural networks (NNs) dominate the curse of dimensionality by sacrificing on efficiency. Thus, even if DQN can manage more complex tasks than \textit{simple} and tabular methods such as QL, it is extremely slow in comparison on those less sophisticated methods. 

\subsection{PPO}

\textbf{PPO} (Proximal Policy Optimization) \cite{pmlr-v115-wang20b} is a policy gradient method that improves upon standard policy optimization by ensuring stable updates through a clipped surrogate objective. The clipped objective function is:

\[
L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( r(\theta) \hat{A}, \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A} \right) \right]
\]

$r(\theta) = \frac{\pi_\theta(s, a)}{\pi_{\theta_{\text{old}}}(s, a)}$ is the probability ratio, and $\hat{A}$ is an estimate of the advantage function. The clipping prevents large policy updates that could degrade performance, striking a balance between exploration and stability.

PPO can be seen as a first-order approximation to Trust Region Policy Optimization (TRPO) \cite{schulman2015trust}, with significantly reduced complexity. It uses multiple epochs of minibatch updates with data collected from a single policy rollout, making it highly sample-efficient and practical for large-scale problems.
PPO is widely used in continuous control tasks, robotics and real-world RL applications due to its robustness and effectiveness.
However, PPO as other methods employing NNs are way slower than simpler algorithms and may not be recommended (or feasible) on small hardware applications.

\section{Design Considerations}

Designing an effective reinforcement learning (RL) system involves careful planning of both the environment in which the agent operates and the structure of the experiences through which it learns. Proper design directly affects learning efficiency, convergence, generalization and real-world applicability.

\subsection{Environment Design}

\paragraph{The state space}

The state space can contain continuous values as well as discrete values. For example, a chess environment will use discrete values for representing the cells whereas a cart pole \cite{nagendra2017comparison} simulator may use a continuous value for the rotation angle. In general it depends on the domain element that is being simulated but it also depend on the degree of realism of the simulation. A pedestrian's position can be represented either with a discrete cell coordinates on a grid or with floating point coordinates.

\paragraph{The observation function}

The environment exposes its state (often only partially observable, such as in a Poker simulation), the observation function takes this state and transforms it into the observation that is fed into the agent.
The state can be left as is or transformed, depending on the agent architecture but also on other factors. For example, if the state contains continuous values and the agent is driven by a tabular learning model (which gives its best discrete inputs), the observation function could discretize the state using \textit{quantization} or \textit{feature-extraction}.

With \textbf{quantization}, a continuous value is mapped into a discrete value by transforming the value to the ``nearest'' discrete value. For example, the decimal number $3.14$ could be \textit{truncated} to $3$ or rounded to $4$. Often is it also necessary to scale the value, which can be integrated in the quantization process relatively easily, as shown in Equation {}. $D_{min}$ and $D_{max}$ are the domain max/min values, $C_{min}$ and $C_{max}$ are the min/max values of the discrete co-domain.

\begin{equation}
  \label{}
  quant(x) =
  \left{
    \begin{split}

    \begin{split}
  \right}
\end{equation}

      On the other hand, \textbf{feature-extraction} extracts a ``meaningful'' discrete value from a ``raw'' continuous value. Given an environment for Blackjack, the state are the cards on the desk (or their values). The observation function could ``extract'' a discrete feature $x_f$ which represents whether or not the player's pile has reached the sum of $14$, thus a ${0,1}$ discrete value. 


\paragraph{The action space}

The action space $\mathcal{A}$ defines what the agent is capable of doing at any given state. This can be discrete (e.g., move left/right) or continuous (e.g., apply force vector). The granularity of the action space affects both the difficulty of the learning task and the precision of control. Proper discretization or abstraction is often required for effective learning.
\paragraph{The reward function}

The reward function $R(s, a, s')$ drives learning by quantifying the desirability of state transitions. A well-shaped reward encourages the intended behavior and reduces exploration ambiguity. Sparse or poorly aligned rewards can result in slow convergence or unintended strategies. Reward shaping, penalties, and potential-based methods are common tools to refine reward signal quality.

\subsection{Experience Design}

Beyond the environment, how the agent accumulates and uses experience plays a pivotal role in learning success. Experience design strategies aim to scaffold or accelerate the agent’s learning journey.

\paragraph{Frankenstein learning}

Frankenstein learning combines experiences, components, or behaviors learned in different contexts into a unified policy or model. This approach is useful in modular or multi-task settings where different capabilities are developed in isolation and later integrated.

\paragraph{Incremental learning}

Incremental learning gradually introduces task complexity, allowing the agent to build up its capabilities step by step. This method avoids overwhelming the agent early on and supports more stable learning trajectories, especially in high-dimensional or long-horizon tasks.

\paragraph{Curriculum learning}

Curriculum learning arranges the training experience in a sequence of increasingly difficult tasks or environments. Inspired by human learning, this strategy improves convergence and generalization by guiding exploration and avoiding premature failures.

\paragraph{Real-time learning}

In real-time learning, the agent updates its policy on-the-fly as it interacts with the environment, often under latency or performance constraints. This setup is common in robotics and real-world systems, where delayed or batch learning is infeasible. Ensuring stability, responsiveness, and safety is critical.

\paragraph{Simulated learning}

Simulated learning allows the agent to explore and train in a virtual environment, often at high speed and without physical risk. While enabling rapid experimentation, the main challenge lies in the simulation-to-reality gap. Techniques such as domain randomization and fine-tuning help bridge this gap and improve transferability.
