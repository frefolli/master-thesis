\chapter{Background on Reinforcement Learning}

This chapter introduces the concept of Reinforcement Learning, along with its differences from traditional machine learning approaches, its advantages and disadvantages, and its mathematical foundation.

\section{A new perspective}

Machine Learning (ML) is a branch of Artificial Intelligence concerning all those statistical techniques and algorithms that can be used to learn a task from an existing data and generalize this knowledge to unseen data. Depending on how the agents learns and what task it learns, ML can be divided into three paradigms: supervised, unsupervised and reinforcement learning.

\paragraph{Learning by example}

Supervised learning (SL) is perhaps the most intuitive and widely used learning paradigm.
The agent is given an input and returns an output. Its output is then compared with the expected output (provided along with the input) using a \textit{Loss Function}, which measures the error between the two values.
This loss is then used to update the agent's model in order to improve its accuracy.
Since the quality of the agent is defined by the similitude between its output and the reference (``correct'') output, it is essentially learning to replicate a behaviour of which the labelled dataset is an example.

Despite its simplicity, this paradigm take two strong assumptions.
1) The current output of the agent doesn't influence the goodness of a future output. That is to say: each decision or value emitted from the agent is independent temporally from other values emitted over time.
Tasks involving classification or labelling, such as identifying the class of a dry bean or spam detection, fall naturally into this category.
2) A dataset of input-output pairs is available and, more importantly, a correct output value exists and it is priorly known or computable. This restriction automatically excludes all problems involving uncertainty, long-term consequences, or scenarios in which the optimal decision depends on a sequence of past and future actions. However, it accommodates a lot of problems for which knowledge about past is available, as in prediction tasks like stock forecasting and language models.

\paragraph{Learning by inference}

Unsupervised learning (US) removes the need of labelled data, as the goal here is not to imitate known outcomes but to find hidden structure within the data itself.
Usually, this activity involves partitioning the input dataset into k classes, which result can be then applied to further processing.
Although the ultimate goal is to ensure that items within the same group resemble each other more than they resemble those in other groups, different approaches to achieve such partitioning exist, ranging from distance-based algorithms (such as K-Means) to hashing-based algorithms (such as Locality Sensitive Hashing).
In both cases, the agent is inferring an hidden connection between elements of the dataset using its features.
However, it should be noted that the resulting classification carries no semantic beyond element similarity and, in particular, there is no concept as intrinsic partitioning correctness. Which means that US cannot be applied directly in decision making problems and can only be used for aggregation tasks such as ancestral reconstruction (useful in phylogenetics) and feature extraction.

\paragraph{Learning by experience}

A useful analogy comes from early human development. Consider a toddler learning to walk. The parent may encourage the child by standing across the room and using gestures or facial expressions (such as smiles or glares) to signal success or failure. The toddler does not receive explicit instructions or labelled examples of correct foot placement. Instead, learning unfolds through trial and error, with the child gradually discovering which patterns of movement lead to praise or comfort. Reinforcement learning (RL) mirrors this process: knowledge is acquired not through imitation or discovery of structure, but through experience and consequence.

The agent is acknowledged to lie into an environment, which has an internal logic and an internal state but those are not exposed to the agent.
The agent can only observe the environment with an \textit{Observation Function}, which returns a partial view (``observation'') of its state.
This observation is then used by the agent's decision-making process (``policy'') to take an action.
The environment updates its state with a \textit{Transition Function} which applies the effects of the agent's action to the current state and computes the next state.
The agent then collects the new observation and the reward released by the \textit{Reward Function} in order to improve its action.

Therefore, instead of learning from a static dataset, the agent learns by the experience gained when interacting with the environment.
Another key intuition is that the focus of RL is not on the single action, but on the outcome of a sequence of interactions. Thus, the agents aims to maximize the cumulative reward.
For that reason, RL is applicable in all those tasks in which the quality of an action is defined also by past and future actions, such as traffic light management.

\section{Mathematics foundation of Environment Modeling and Reinforcement Learning}

\subsection{Markov Chains}

The evolution of RL environment could be roughly described as a \textbf{Markov Chain}, which is is a tuple $MC = (S, P)$ where $S$ is the set of all states and $P$ is a transition function such that $P(S_n | S_{n-1})$ denotes the probability of the state $S_n$ happening given that the previous state is $S_{n-1}$. An example of Markov chain is shown in figure \ref{fig:markov-chain}. Intuitively, any episode of an environment is a sequence of states and there is some kind of transition function which moves the environment from one state to another.

Notably, an important property of Markov Chains is that \textit{past and future are mutually independent}. This may sound like a contradiction to the concept of Markov Chain, but it is in fact a simplification over statistics which allows to model an environment more easily. The appearance of the current state is assumed to be influenced only by the appearance of the previous state. This principle can be summarized as $P(S_n | S_{n-1}) = P(S_n | S_{n-1}, S_{n-2}, S_{n-3}, S_{n-4})$.

Markov chains are a powerful tool for modelling stochastic environments, such as weather conditions. Supposing to have a dataset of daily weather (sunny, cloudy, rain, thunderstorm), one could compute an approximation of $P(S_n | S_{n-1})$ and use it for weather forecasting.

Now, recalling the Markov property and seeing this implementation, a valid argument against this practise of course could be that one precedent weather record isn't enough for predicting future records with sufficient accuracy. But states can also be aggregate or complex objects: Markov chains allow defining a state as the previous k records of weather data.

A more subtle detail resides in what it actually models: a Markov chain doesn't take into account the \textit{external forces} (agents' actions). For example, whether or not the Government decides to start \textit{cloud seeding} is already a game changer variable which radically changes the weather model.

\putimage{figures/markov-chain.png}{An example of three state Markov Chain}{fig:markov-chain}{0.5}

\subsection{Markov Decision Processes}

A \textbf{Markov Decision Process} is defined a tuple $MDP = (S, A, P, R)$ where S is the set of all states, $A$ is the set of all actions, $P$ is a transition function such that $P(S_n | S_{n-1}, a)$ denotes the probability of the state $S_n$ happening given that the previous state is $S_{n-1}$ with the action $a$, and $R$ is a reward function such that $R(S_{n-1}, a, S_{n})$ denotes the immediate reward of the transition from state $S_{n-1}$ to state $S_n$ with the action $a$. An example of MDP is shown in figure \ref{fig:markov-decision-process}.

This model not only accounts for agents' actions, but also provides an abstraction for expressing the quality of an action or the desirability of a certain state transition.

\putimage{figures/markov-decision-process.png}{The expansion of the Markov Chain of figure \ref{fig:markov-chain} as a Markov Decision Process. Green transitions release a positive reward, red transitions release a negative one.}{fig:markov-decision-process}{0.5}

\subsection{Policies}

The agent behaviour in a MDP environment can be defined with a \textbf{policy} $\pi$, which is a deterministic or stochastic mapping from the state set to the action set.
A deterministic policy $\pi_{det}$ is a function $\pi_{det} \; : \; S \rightarrow A$, while a stochastic policy $\pi_{sto}$ is a function $\pi _ {S} \; : \; S \times A \rightarrow [0, 1]$, denoted as $\pi _ {S}(a | s)$, where $\sum_{a \in A} \pi _ {sto} (s, a) = 1$.

The behaviour of an agent can then be evaluated by evaluating its policy: a policy $\pi _ a$ is \textit{better} than a policy $\pi _ b$ if the accumulated reward over time of $\pi _ a$ is higher than the one of $\pi _ b$.
In particular, a policy $\pi _ o$ is defined \textit{optimal} if $\forall \pi _ i \neq \pi _ o$, it holds that $\pi _o $ is \textit{better} than $\pi _ i$.
If the relative quality of a policy if defined by a scalar value, how can the accumulated rewards be computed?

\subsection{Value Functions}

\paragraph{State-value function}
\label{par:state-value-function}

The \textbf{state-value} function $V^{\pi}(s)$ evaluates the amount of expected reward obtained by following a policy $\pi$ from the state $s$.
Since both the environment and the agent's policy can be stochastic, an abstract formula to compute this concept reuses the expected value operator $\mathbb{E}$ from probability theory.
An slice of the episode of an MDP can be represented as a process graph starting from a node $s_i$, developing into a chain of states and actions. The value of this computation is given by a weighted sum of the rewards of tis transitions.
Since the same node and action can lead to different nodes, the process becomes a sequential, ramified structure as all possible transitions are recursively considered.
The expected reward of the state $s_i$ can be thought as the weighted sum of rewards of the average path along this ramified process, which leads to the following formula:

\begin{equation} \label{eq:v-pi-E}
V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t + 1}) \mid s_0 = s \right]
\end{equation}

Here, \textbf{$\gamma$} is discount factor used for weighting the sum of gained rewards. If $0 < \gamma < 1$, then $lim ^ {t \rightarrow +\infty} \gamma ^ t = 0$ and the immediate rewards have priority over long-term rewards. Otherwise, if $\gamma > 1$, then $lim ^ {t \rightarrow +\infty} \gamma ^ t = +\infty$ and the long-term rewards have priority over immediate rewards. The value of $\gamma$ should reflect the desirability of state transitions and the end goal of the agent learning in this environment. For this reason, it may also be the case for $\gamma = 1$ if immediate rewards and long-term rewards have the same importance in defining the goodness of the agent behaviour. For practical reasons, the abstract $V^{\pi}(s)$ formula can be rewritten to a more manageable recursive form:

\begin{equation} \label{eq:v-pi-bellman}
V^{\pi}(s) = \sum _ {a \in A} \pi (a | s) \sum _ {s' \in S} P(s' | s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right]
\end{equation}

\paragraph{Action-value function}
\label{par:action-value-function}

The \textbf{action-value} function $Q^{\pi}(s, a)$ evaluates the amount of expected reward obtained by following a policy $\pi$ from the state $s$ using the action $a$.
Reusing the reasoning of Subsection \ref{par:state-value-function}, the abstract formula can be written as:

\begin{equation} \label{eq:Q-pi-E}
Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1}) \mid s_0 = s, a_0 = a \right]
\end{equation}

Again, for practical reasons, the abstract $Q^{\pi}(s, a)$ formula can be rewritten to a more manageable recursive form:

\begin{equation} \label{eq:Q-pi-bellman}
Q^{\pi}(s, a) = \sum _ {s' \in S} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum _ {a' \in A} \pi (a' | s') Q^{\pi}(s', a') \right]
\end{equation}

\paragraph{Usefulness of value functions}

Both the \textit{state-value} function $V^{\pi}(s)$ \ref{par:state-value-function} and the \textit{action-value} function $Q^{\pi}(s, a)$ \ref{par:action-value-function} allow to compare different policies in order to find the most advantageous one at a certain point of time. Moreover, putting $s = s_0$ also allows to obtain an overall picture of quality of a policy. However, the function $Q^{\pi}(s, a)$ is also useful when constructing a winning policy. If a good approximation of $Q^{\pi}(s, a)$ is known, then the agent can choose the best performing action $a_o \in A$ using a simple comparison: $\forall a \in A \; | \; Q^{\pi}(s, a) \leq Q^{\pi}(s, a_o)$.

\section{Learning to learn}

\subsection{Learning techniques}

\paragraph{Model-Based/Free learning}

A \textit{model} is a representation of the internal logic of the Markov Decision Process. This model can also be a learned approximation. In fact, all algorithms in which the agent learns this representation are called \textbf{model-based}. Conversely, a \textbf{model-free} algorithm doesn't estimate the transition probabilities and reward function of the MDP. Instead, it directly learns an approximated optimal policy through trial and error, requiring no prior knowledge on the environment other than the shapes of the state and action sets.
In the present research, the focus is on \textit{model-free} learning agents, as the used RL algorithms will be explained in further sections.

\paragraph{On/Off-Policy learning}

As the model-free agent should gain experience interacting with environment, a question arise? Which policy should it use? A \textbf{on-policy} algorithm uses the currently known best policy which is being updated at every interaction. A \textbf{off-policy} algorithm uses the consolidated optimal policy to interact with the environment and gain experience (tuples state-action-reward, for example) which will be used at a later time to update the consolidated policy, building the new optimal policy. It is also possible to use a \textbf{on-policy} strategy combined with a greedy exploration of the state-action cross space.

For example, the \textbf{Epsilon-Greedy} (EG) balances exploration (finding variants of a policy) and exploitation (using the optimal policy) phases using an decaying $\epsilon$ coefficient.
Every time an inference is needed, the EG algorithm peeks a random decimal number $x \in [0, 1]$. If $x \leq \epsilon$ then is chooses randomly an action to perform, otherwise it selects the current best action given by the policy.
Over time the $\epsilon$ coefficient lowers using a decaying factor $d$ so that $\epsilon_{t + 1} = \epsilon_{t} d$. This simple but effective algorithm allows to \textit{on-policy} algorithms (and also \textit{off-policy} ones) to explore the state-action space, since they otherwise risk of getting stuck to local optimums.
An important notice is that the EG algorithm is not by any degree a reinforcement learning algorithm by itself, it only enhance other RL algorithms.

\paragraph{Monte Carlo methods}

A \textbf{Monte Carlo} (MC) method exploits random exploration and averages the complete rewards obtained by those paths to update the approximation of the value functions.
As an example consider a Chess board in a given configuration $s_t$ at a time $t$. From this state there are multiple possible actions. A MC approach explores each one (as shown in Figure \ref{fig:monte-carlo-tree-chess}) with $k$ random walks and then computes averages in order to updates its action/state-value function approximation.

The Monte Carlo method is historically an impactful evolution of exhaustive recursive search which used a similar approach for exploring the state-action space but not randomness-driven. As can be easily imagined, many "games" such as Chess have a gazillion of possible states, and in general if the number of possible actions at each state is k, then number of possible state-action sequences after n moves is $\Omega(k^n)$. It should be noted that in case of stochastic environment, the number of possible paths is likely much higher. Using random walks and averaging results, MC manages to dominate the \textit{curse of dimensionality} while maintaining a decent accuracy.

Usually updates happen on a episode-by-episode basis, using only the cumulative reward for averages. Thus, MC doesn't account for distance in time of immediate rewards and in general it doesn't case of short-term rewards. As a consequence, MC is unable to update the value function for intermediate states.

\putimage{figures/monte-carlo-tree-chess.png}{An example of Monte Carlo random sampling for Chess}{fig:monte-carlo-tree-chess}{0.5}

\paragraph{Temporal Difference methods}

\subsection{Environment design}

\paragraph{The state space}

\paragraph{The action space}

\paragraph{The observation function}

\paragraph{The reward function}

\subsection{Experience design}

\paragraph{Frankenstein learning}

\paragraph{Incremental learning}

\paragraph{Curriculum learning}

\subsection{Reinforcement Learning Algorithms}
