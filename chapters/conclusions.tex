\chapter{Conclusions}

This chapter summarizes the main contributions and findings of the present work, a discussion over threads to validity and an outline of possible future works.

\section{Thesis contributions}

Through various experiments, this thesis investigated the application of popular concepts such as curriculum learning, multi agent learning, self-adaptive systems to traffic light control and traffic calming.
These are supported by the SUMO-RL framework, which has been developed as a fork of the original SUMO-RL by Lucas Alegre in order to enhance its capabilities, increase its performance and overcome its limits.
The extended framework now supports a total of $4$ observation function, $5$ reward function, $3$ tabular-based RL agents, $2$ deep RL agents, the fixed cycle agent with $4$ cycle programs and the ability to run priority-based unregulated intersections (as comparison).
Moreover SUMO-RL allows to use Multi Agent Learning techniques such as sharing rewards and state, allocate more than one intersection to an agent and is equipped with a self-adaptive algorithm for letting a deployed agent learn when performances are degrading.

The work began by establishing a solid baseline for RL agents, identifying the most effective reward and observation functions, which serve as a foundation for subsequent experiments.
The best observation function is based on lane density, which is good because it is fairly easy to detect with real-world equipment, saving up to $\frac{2}{3}$ of waiting times.
The best reward function is based on the derivate of accumulated waiting times, saving up to $\frac{2}{3}$ of waiting times.
Although it has achieves better results, transit authority should go for more easy metrics to track such as queue lengths or the derivate of queue lengths, which were proved experimentally to be similar in performance to the best reward function.

Building on this, this research compared different ways of building datasets for training traffic light agents.
Curriculum learning was successfully applied in its combinatorial interpretation in order to simplify the structure of training episodes as it achieves the same performance levels of the monolithic one which exhaustively presents the agent with almost all possible traffic conditions (driven by a traffic registry).
The insertion of disruptive time slots in training episodes is shown to improve agent resilience in challenging traffic conditions, highlighting the benefits of structured exposure to variability.
On the other way round, the lack of intensive episodes doesn't stimulate enough the agent and worsen all the metrics.

A comparison of agents, going from classic tabular reinforcement learning algorithms such as Q Learning to popular neural network algorithms such as Proximal Policy Optimization, was drawn to determine if neural networks are really that useful in traffic management context or if a simpler tabular approach can replace them.
The learning agents were also compared with fixed cycle traffic lights and unregulated intersection, verifying not only that the PPO implementation achieves the same performance as the best fixed cycle program ($15$ seconds per phase), but more importantly that the Q Learning based agents can easily learn a policy so good that waiting times are literally cut in half with respect of the former agents.
This study has also proven the better efficiency of short cycle program with respect to longer cycle programs, which explains why naturally fast paced traffic lights feel smoother.

This research has analyzed the sensitivity of learning agents to various hyperparameters.
In particular, small sized quantization has been confirmed to be more effective in tabular agents, which is an expected result due to the fact that those are essentially dictionaries of state and actions, bigger quantization means more state space to explore and longer training times.
Moreover, the non-determinism behaviour often implemented in reinforcement learning models has proven to be useful not only in training phases but also in evaluation phases since it boosts significantly performances.
Because Markov Decision Processes and value-function approaches are probabilistic by nature, this result should not be unfamiliar.

%TODO: MARL vs SARL
%TODO: SA vs STATIC

\section{Threads to validity}

\section{Future Works}
