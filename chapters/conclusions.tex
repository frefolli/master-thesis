\chapter{Conclusions}
\label{chapter:conclusions}

This chapter summarizes the main contributions and findings of the present work, a discussion over threads to validity and an outline of possible future works.

\section{Thesis contributions}

Through various experiments, this thesis investigated the application of popular concepts such as curriculum learning, multi agent learning, self-adaptive systems to traffic light control and reinforcement learning.
These are supported by the SUMO-RF framework, which has been developed as a fork of the original SUMO-RL by Lucas Alegre in order to enhance its capabilities, increase its performance and overcome its limits.
The extended framework now supports a total of $4$ observation function, $5$ reward function, $3$ tabular-based RL agents, $2$ deep RL agents, the fixed cycle agent with $4$ cycle programs and the ability to run priority-based unregulated intersections (as comparison).
Moreover SUMO-RF allows to use Multi Agent Learning techniques such as sharing rewards and state, allocate more than one intersection to an agent and is equipped with a self-adaptive algorithm for letting a deployed agent learn when performances are degrading.

The work began by establishing a solid baseline for RL agents, identifying the most effective reward and observation functions, which serve as a foundation for subsequent experiments.
The best observation function is based on lane density, which is good because it is fairly easy to detect with real-world equipment, saving up to $\sim 50\%$ of waiting times.
The best reward function is based on the derivative of accumulated waiting times, saving up to $\sim 60\%$ of waiting times.
Although it has achieves better results, transit authority should go for more easy metrics to track such as queue lengths or the derivative of queue lengths, which were proved experimentally to be similar in performance to the best reward function.

Building on this, this research compared different ways of building \gls{gl:dataset}[s] for training traffic light agents.
Curriculum learning was successfully applied in its combinatorial interpretation in order to simplify the structure of training episodes as it achieves the same performance levels of the monolithic one which exhaustively presents the agent with almost all possible traffic conditions (driven by a traffic registry).
The insertion of disruptive time slots in training episodes is shown to improve agent resilience in challenging traffic conditions, highlighting the benefits of structured exposure to variability.
On the other way round, the lack of intensive episodes doesn't stimulate enough the agent and worsen all the metrics.

A comparison of agents, going from classic tabular reinforcement learning algorithms such as Q Learning to popular neural network algorithms such as Proximal Policy Optimization, was drawn to determine if neural networks are really that useful in traffic management context or if a simpler tabular approach can replace them.
The learning agents were also compared with fixed cycle traffic lights and unregulated intersection, verifying not only that the PPO implementation achieves the same performance as the best fixed cycle program ($15$ seconds per phase), but more importantly that the Q Learning based agents can easily learn a policy so good that waiting times are literally cut in half with respect of the former agents.
This study has also proven the better efficiency of short cycle program with respect to longer cycle programs, which explains why naturally fast paced traffic lights feel smoother.

This research has analyzed the sensitivity of learning agents to various hyperparameters.
In particular, small sized quantization has been confirmed to be more effective in tabular agents, which is an expected result due to the fact that those are essentially dictionaries of state and actions, bigger quantization means more state space to explore and longer training times.
Moreover, the non-deterministic behaviour often implemented in reinforcement learning models has proven to be useful not only in training phases but also in evaluation phases since it boosts significantly performances.
Because Markov Decision Processes and value-function approaches are probabilistic by nature, this result should not be unfamiliar.

The Multi Agent Learning approaches based on observation/reward sharing has been subject of experiments in which the baseline has been compared with the various shared view functions.
Sharing the current traffic light phase to neighbours has been proven to be beneficial for agents with increased throughput and decreased waiting times.
Also sharing rewards in the form of derivative of accumulated waiting time is beneficial and a sensitivity analysis revealed that summing raw neighbourhood rewards is just as effective as applying a value dumping (that is, a weighted sum), which is a considerable detail.

Finally, this work has investigated the effectiveness of self-adaptive strategies applied to reinforcement learning, examining whether agents are able to autonomously update their policies in response to changing conditions.
A self-adaptive algorithm has been implemented but, contrary to expectations, the experimental results reveal that it proves unfeasible in the tested scenarios, as it leads to increased waiting times and reduced throughput compared to non-adaptive methods due to the distortions that learning induces.

\section{Threats to validity}

This study is affected by limitations driven by design choices, performance constraints and available resources.
However, several mitigation actions were took in order to balance the trade-off between feasibility and realism, aligning with practices commonly found in the literature or establishing claim boundaries.

\paragraph{Internal validity}

Despite SUMO provides a limited degree of realism, it is rather slow, especially when used through TraCI.
Therefore it was configured in the most lightweight possible in order to rationalize the usage of computational resources.
This means having to deal with unrealistic phenomena such as lane changes being instant or vehicles interpenetrating on collision.
Nonetheless, this low fidelity nature was mitigated with flow configuration tweaks in order to approximate real-world dynamics, such as simulating artificial queues at network border and allowing vehicles to use arbitrary lanes on entrance.

Unlike most works in this field, in this thesis observation functions were deliberately chosen to produce a small state space because the interest was on determining from which features agents benefit the most.
Interestingly, results support this decision because a simple \textit{Density} function outperforms the richer \textit{Default} function, which resembles the ones often used in literature, suggesting that smaller state representations can be more effective than a more complete one.

Moreover, training was limited to four episodes, a smaller number than commonly used in the literature.
However, each episode lasted 100,000 simulated seconds, giving agents exposure to extended traffic patterns, and effective learning was still achieved.

Also the range of models tested was limited to the base forms of reinforcement learning algorithsm without advanced architectures such as transformers.
Nevertheless, three tabular agents (SARSA, Q-Learning, Double Q-Learning) and two neural ones (DQN, PPO) were implemented, and sensitivity analyses were carried out on key hyperparameters to ensure fair evaluation.
Again, the aim was verifying the potential of different approaches with limited resources.

\paragraph{External validity}

The scenarios employed were restricted to small networks due to computational constraints and TraCI inefficiency.
This choice narrows the generalizability of the findings, although it is in line with the scale of experiments typically reported in comparable research.
The \gls{gl:dataset}[s] used for training and evaluation are synthetic, therefore not directly induced by real-world originâ€“destination matrices (OD), but they were constructed to reflect realistic traffic phenomena, such as peak cycles and directional commuting flows, which are often absent from the highly simplified \gls{gl:dataset}[s] used in related works.
Publicly available OD data are either too large to simulate with available resources or too sparse to reconstruct meaningful flows.

\section{Future Works}

With adequate equipment and time it would be important to perform experiments on broader networks and with more training episodes to confirm the results obtain in this study.
Future research should investigate \gls{gl:dataset} acquisition strategies alternative to the monolithic or curriculum-based approaches used in this study.
For example, Curriculum Learning can also be interpreted as a set of incrementally difficult task, approach that has been successfully applied in other contexts.
It would be valuable to investigate the effectiveness of \gls{gl:dataset}[s] made of casual traffic with gradually increasing intensity.
Further investigations should also address transfer learning, testing whether an agent trained to manage an intersection of a given size (e.g., with $N$ lanes) can be adapted effectively to intersections of different sizes (e.g., with $M \ne N$ lanes).
On the same line, it would be interesting to verify with future experiments whether agents trained jointly can later be swapped across different networks or scenarios without significant performance degradation, to test their degree of modularity.
The degree of generalization of using a intersection allocation strategy, such as partitioning them by state space size, could also be investigated.
In other words, whether an agent controlling multiple intersections simultaneously can outperform configurations where each intersection is managed by a distinct agent.
Finally, it would be worth exploring heterogeneous learning environments, where a trained agent (Magister) operates alongside an untrained agent (Studens).
Such experiments could reveal whether the less experienced agent benefits from the presence of the more skilled one, and whether the latter suffers from the inexperience of its partner.
All these directions can be supported by the current version of SUMO-RF which already allows almost all of these variations without changes.
