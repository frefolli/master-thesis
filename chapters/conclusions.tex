\chapter{Conclusions}

This chapter summarizes the main contributions and findings of the present work, a discussion over threads to validity and an outline of possible future works.

\section{Thesis contributions}

Through various experiments, this thesis investigated the application of popular concepts such as curriculum learning, multi agent learning, self-adaptive systems to traffic light control and reinforcement learning.
These are supported by the SUMO-RL framework, which has been developed as a fork of the original SUMO-RL by Lucas Alegre in order to enhance its capabilities, increase its performance and overcome its limits.
The extended framework now supports a total of $4$ observation function, $5$ reward function, $3$ tabular-based RL agents, $2$ deep RL agents, the fixed cycle agent with $4$ cycle programs and the ability to run priority-based unregulated intersections (as comparison).
Moreover SUMO-RL allows to use Multi Agent Learning techniques such as sharing rewards and state, allocate more than one intersection to an agent and is equipped with a self-adaptive algorithm for letting a deployed agent learn when performances are degrading.

The work began by establishing a solid baseline for RL agents, identifying the most effective reward and observation functions, which serve as a foundation for subsequent experiments.
The best observation function is based on lane density, which is good because it is fairly easy to detect with real-world equipment, saving up to $\sim 50\%$ of waiting times.
The best reward function is based on the derivative of accumulated waiting times, saving up to $\sim 60\%$ of waiting times.
Although it has achieves better results, transit authority should go for more easy metrics to track such as queue lengths or the derivative of queue lengths, which were proved experimentally to be similar in performance to the best reward function.

Building on this, this research compared different ways of building datasets for training traffic light agents.
Curriculum learning was successfully applied in its combinatorial interpretation in order to simplify the structure of training episodes as it achieves the same performance levels of the monolithic one which exhaustively presents the agent with almost all possible traffic conditions (driven by a traffic registry).
The insertion of disruptive time slots in training episodes is shown to improve agent resilience in challenging traffic conditions, highlighting the benefits of structured exposure to variability.
On the other way round, the lack of intensive episodes doesn't stimulate enough the agent and worsen all the metrics.

A comparison of agents, going from classic tabular reinforcement learning algorithms such as Q Learning to popular neural network algorithms such as Proximal Policy Optimization, was drawn to determine if neural networks are really that useful in traffic management context or if a simpler tabular approach can replace them.
The learning agents were also compared with fixed cycle traffic lights and unregulated intersection, verifying not only that the PPO implementation achieves the same performance as the best fixed cycle program ($15$ seconds per phase), but more importantly that the Q Learning based agents can easily learn a policy so good that waiting times are literally cut in half with respect of the former agents.
This study has also proven the better efficiency of short cycle program with respect to longer cycle programs, which explains why naturally fast paced traffic lights feel smoother.

This research has analyzed the sensitivity of learning agents to various hyperparameters.
In particular, small sized quantization has been confirmed to be more effective in tabular agents, which is an expected result due to the fact that those are essentially dictionaries of state and actions, bigger quantization means more state space to explore and longer training times.
Moreover, the non-deterministic behaviour often implemented in reinforcement learning models has proven to be useful not only in training phases but also in evaluation phases since it boosts significantly performances.
Because Markov Decision Processes and value-function approaches are probabilistic by nature, this result should not be unfamiliar.

The Multi Agent Learning approaches based on observation/reward sharing has been subject of experiments in which the baseline has been compared with the various shared view functions.
Sharing the current traffic light phase to neighbours has been proven to be beneficial for agents with increased throughput and decreased waiting times.
Also sharing rewards in the form of derivative of accumulated waiting time is beneficial and a sensitivity analysis revealed that summing raw neighbourhood rewards is just as effective as applying a value dumping (that is, a weighted sum), which is a considerable detail.

Finally, this work has investigated the effectiveness of self-adaptive strategies applied to reinforcement learning, examining whether agents are able to autonomously update their policies in response to changing conditions.
A self-adaptive algorithm has been implemented but, contrary to expectations, the experimental results reveal that it proves unfeasible in the tested scenarios, as it leads to increased waiting times and reduced throughput compared to non-adaptive methods due to the distortions that learning induces.

\section{Threats to validity}

% Internal Threats
% - SUMO simulation model has a low degree of realism and the most lightweight options were used in simulation. But often works of the same type do the same and here we at least tried to introduce realistic phenomena like artificial queues at the border of the scenario.
% - We tried no advanced Deep Learning model, nor transformer based architecture, nor advanced tweaks on those models. However we tried three different tabular models (SARSA, QL and Double QL), two neural models (DQN and PPO) done a sensitivity analysis on many hyperparameters such as quantization, epsilon greedy curves, buffer size, entropy regularization coefficient and so on in order to get the max out of it.
% - Other works in the area use huge observation functions which supply an agent with a lot of data such as current phase, lane densities, lane queue lengths and so on. We tried to keep things as small as possible by providing a "Default" function which is exactly that mixture and other observation functions with just one metric such as lane density, lane queue lengths, lane average speeds, ... etc. Moreover, results shown that the much simpler "Density" function bulldozes the "state of art" observation function "Default".
% - Rewards computed by reward functions use a dumping so that their values remain tractable, because extreme reward signals are known to cause training instability.
% - The training process had 4 episodes which is considered low according to literature (usually goes up to 100) but our episodes last for 100k seconds each which is a lot. Therefore an agent get to train for the equivalent of almost a week, which we think is enough to pronounce conclusions (supported by evidence because the DQL agent is able to learn a policy which with only 4 episodes).
% - The dataset is synthetic and it is subject to biases, but it was created following realistic phenomena like peak/off-peak cycles and time-of-day directions (home -> work and in evening work -> home). Other works often rely also on synthetic data but some times is very simplistic (like NS vs EW equal flows or only in one direction).
% External Threats
% - In experiment we used tiny scenarios with a few intersection but it is limited both by computational resources needed and by TraCI inefficiency. Moreover, other works often don't used networks more sophisticated than ours.
% - The datasets are synthetic and not anchored to a real world Origin/Destination matrix. This is due to the fact that transit authorities that public traffic data either public OD matrices for huge networks which are too big to be simulated with resources available or provide insufficient data consistent of only single vehicles which are not enough to reconstruct a consistent flow.

The limitations of this research activity are acknowledged as threats to internal validity, affecting design choices and the ability to support claims, and threats to external validity, affecting the generalizability of conclusions driven by experimental results.
Some measures were adopted to mitigate threats (such as introducing realistic traffic patterns, tuning hyperparameters, and extending episode durations), which supports the credibility of the findings, while also identifying areas for improvement in future research.
Despite these limitations, the design choices made in this thesis balance a trade-off between feasibility and realism, aligning with practices commonly found in the literature.

\paragraph{Internal validity}

SUMO provides only a limited degree of realism, and the most lightweight configuration was adopted to make experiments feasible.
While this reduces fidelity, the choice is consistent with other studies in the field, and additional elements such as artificial queues at the network borders were introduced to approximate real-world dynamics.
The range of models tested also represents a limitation: no advanced deep learning architectures, such as transformers, were explored.
Nevertheless, three tabular agents (SARSA, Q-Learning, Double Q-Learning) and two neural ones (DQN, PPO) were implemented, and sensitivity analyses were carried out on key hyperparameters to ensure fair evaluation.
Observation and reward design also introduce potential biases.
Unlike other works that rely on large observation spaces, this thesis deliberately explored reduced functions.
Results indicate that a simple \textit{Density} function outperforms the richer \textit{Default} function, suggesting that smaller state representations can be effective.
Reward signals were dampened to avoid extreme reward signals which are known to cause training instability.
Training was limited to four episodes, a smaller number than commonly used in the literature.
However, each episode lasted 100,000 simulated seconds, giving agents exposure to extended traffic patterns, and effective learning was still achieved.

\paragraph{External validity}

The scenarios employed were restricted to small networks due to computational constraints and TraCI inefficiency.
This choice narrows the generalizability of the findings, although it is in line with the scale of experiments typically reported in comparable research.
The datasets used for training and evaluation are synthetic, therefore not directly induced by real-world origin–destination matrices (OD), but they were constructed to reflect realistic traffic phenomena, such as peak cycles and directional commuting flows, which are often absent from the highly simplified datasets used in related works.
Publicly available OD data are either too large to simulate with available resources or too sparse to reconstruct meaningful flows.

\section{Future Works}

% Future Works:
% - Try out incremental learning: instead of monolithic or curriculum learning in the form of a typical workday/weekday, I would like to verify if a dataset made of incrementally intensive casual traffic is able to produce better results of the curriculum learning datasets used in this study. The incremental effort interpretation of curriculum learning has been succesfully applied in other works so i would like to compare my dataset with their.
% - Experiment with larger networks.
% - Verify whether agents in a Multi Agent Learning context spontaneously develop a significant degree of modularity, that is: given a set of trained agents with MARL, can i swap them without regret?
% - Verify if an agent controlling more than one intersection can achieve better results than one distinct agent per intersection, where the agent controlling more intersection means that is plays the role of different ones at the same time.
% - Experiment with transfer learning to adapt an already trained agent for a intersection of N lanes to work with an intersection of M != N lanes.
% - Experiment with more training episodes what is the growth rate of knowledge
% - Verify if given a trained agent (Magister) and a newbie agent (Studens) in the same network, if the newbie agent in training gets benefit from being with a more skilled agent and/or if the trained agent is damaged by inexperience of the untrained agent.

With adequate equipment and time it would be important to perform experiments on broader networks and with more training episodes to confirm the results obtain in this study.
Future research should investigate dataset acquisition strategies alternative to the monolithic or curriculum-based approaches used in this study.
For example, Curriculum Learning can also be interpreted as a set of incrementally difficult task, approach that has been successfully applied in other contexts.
It would be valuable to investigate the effectiveness of datasets made of casual traffic with gradually increasing intensity.
Further investigations should also address transfer learning, testing whether an agent trained to manage an intersection of a given size (e.g., with $N$ lanes) can be adapted effectively to intersections of different sizes (e.g., with $M \ne N$ lanes).
On the same line, it would be interesting to verify with future experiments whether agents trained jointly can later be swapped across different networks or scenarios without significant performance degradation, to test their degree of modularity.
The degree of generalization of using a intersection allocation strategy, such as partitioning them by state space size, could also be investigated.
In other words, whether an agent controlling multiple intersections simultaneously can outperform configurations where each intersection is managed by a distinct agent.
Finally, it would be worth exploring heterogeneous learning environments, where a trained agent (Magister) operates alongside an untrained agent (Studens).
Such experiments could reveal whether the less experienced agent benefits from the presence of the more skilled one, and whether the latter suffers from the inexperience of its partner.
All these directions can be supported by the current version of SUMO-RL which already allows almost all of these variations without changes.
