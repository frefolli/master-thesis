\chapter{Experiments and findings}
%% DATASETS ARE GENERATED WITH SEED = 170701

In this chapter, the following research objectives and questions are addressed thoroughly and individually, with related experiments and conclusions.
For the scope of these experiments, if not said otherwise, the experimental setting is based on the Celoria scenario \ref{fig:setting-celoria}.
The datasets are generated using the \textit{generate-datasets} utility with $seed \; = \; 170701$ and the experiments are run automatically by the \textit{executor} module.
All the experiments are executed with the computational resources provided by hpc-ReGAInS@DISCo, which is a MUR Department of Excellence Project within the Department of Informatics, Systems and Communication at the University of Milan-Bicocca \footnote{\href{https://www.disco.unimib.it}{https://www.disco.unimib.it}}.

\hfill \break
\noindent
\textbf{\hypertarget{ro1}{RO 1}: Acquiring a baseline}
\hfill \break
\textit{\hypertarget{rq1.1}{RQ 1.1}: Which is the best implemented reward function?}
\hfill \break
\textit{\hypertarget{rq1.2}{RQ 1.2}: Which is the best implemented observation function?}

\hfill \break
\noindent
\textbf{\hypertarget{ro2}{RO 2}: Evaluating the effectiveness of Curriculum Learning}
\hfill \break
\textit{\hypertarget{rq2.1}{RQ 2.1}: Does curriculum learning leads to better running performances?}
\hfill \break
\textit{\hypertarget{rq2.2}{RQ 2.2}: Is there a loss of knowledge due to curriculum learning not being exhaustive?}
\hfill \break
\textit{\hypertarget{rq2.3}{RQ 2.3}: Does adding disruption episodes in the training set enable agents to tackle conditions?}

\hfill \break
\noindent
\textbf{\hypertarget{ro3}{RO 3}: Comparing Deep Learning with Tabular Learning}
\hfill \break
\textit{\hypertarget{rq3.1}{RQ 3.1}: Which is the best implemented tabular agent?}
\hfill \break
\textit{\hypertarget{rq3.2}{RQ 3.2}: Which is the best implemented neural agent?}
\hfill \break
\textit{\hypertarget{rq3.3}{RQ 3.3}: Is Deep Learning worth its cost?}

\hfill \break
\noindent
\textbf{\hypertarget{ro4}{RO 4}: Comparing Reinforcement Learning with existing solutions}
\hfill \break
\textit{\hypertarget{rq4.1}{RQ 4.1}: What is the impact of cycle time in Fixed Cycle Agents?}
\hfill \break
\textit{\hypertarget{rq4.2}{RQ 4.2}: Are RL Agents an improvement over non-AI solutions?}

\hfill \break
\noindent
\textbf{\hypertarget{ro5}{RO 5}: Evaluating the impact of determinism over performances}
\hfill \break
\textit{\hypertarget{rq5.1}{RQ 5.1}: Is non-determinism helping the agents?}
\hfill \break
\textit{\hypertarget{rq5.2}{RQ 5.2}: Locking determinism does cause a significant drop in performance?}

\hfill \break
\noindent
\textbf{\hypertarget{ro6}{RO 6}: Evaluating the impact of quantization over performances}
\hfill \break
\textit{\hypertarget{rq6.1}{RQ 6.1}: Do agents benefit from a small quantization?}
\hfill \break
\textit{\hypertarget{rq6.2}{RQ 6.2}: Do continuous-space agents benefit from disabling quantization?}

\hfill \break
\noindent
\textbf{\hypertarget{ro7}{RO 7}: Comparing Multi-Agent Learning with Single-Agent Learning}
\hfill \break
\textit{\hypertarget{rq7.1}{RQ 7.1}: Does sharing rewards to neighbour agents improve globally the solution?}
\hfill \break
\textit{\hypertarget{rq7.2}{RQ 7.2}: Does sharing observations to neighbour agents improve globally the solution?}

\hfill \break
\noindent
\textbf{\hypertarget{ro8}{RO 8}: Evaluating the effectiveness of Self-Adaptive strategies}
\hfill \break
\textit{\hypertarget{rq8.1}{RQ 8.1}: Can a reinforcement learning system be designed to update itself as needed?}
\hfill \break
\textit{\hypertarget{rq8.2}{RQ 8.2}: Is the self-adaptive system improving performance over time?}
\hfill \break
\textit{\hypertarget{rq8.3}{RQ 8.3}: Is the self-adaptive system updating as intended?}

% \hfill \break
% \noindent
% \textbf{\hypertarget{ro9}{RO 9}: Assessing the impact of agent allocation strategies}
% \hfill \break
% \textit{\hypertarget{rq9.1}{RQ 9.1}: Does using an agent controlling multiple intersections of the same type achieve better traffic performance than an agent per intersection?}
% \hfill \break
% \textit{\hypertarget{rq9.1}{RQ 9.1}: Are atomic agents less modular than abstract agents?}

\section{Acquiring a baseline}
  
  Before even evaluating novel approaches in Reinforcement Learning it is useful to investigate which are the best base settings. This involves finding the best Reward function and the best Observation function among the implemented set of functions. To optimize these two hyperparameters, two distinct experiments have been performed in parallel, each one comparing the various implemented functions.

  The QLAgent has been trained for $400k$ virtual seconds with a \textbf{monolithic} dataset containing each one of the entries of the Traffic Registry. It is then evaluated for another $400k$ virtual seconds using an evaluation dataset composed of four possible everyday scenarios comprising both low traffic and high congestions. From a technical standpoint, this complete dataset has been generated with the following schema using the \textit{generate-flow/dataset} utility:

\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[language=JSON, caption=The schema for the "Monolithic" Dataset, label={lst:dataset-schema-monolithic}, mathescape=true]
"training": [
  "4,100000,$\pounds$,*,$\sim$"
], "evaluation": [
  "1,\pounds,N1,N2,N1,N3,N1",
  "1,\pounds,N1,N4,N1,N5,N1",
  "1,\pounds,N1,STPL2,N1,STPL3,N1",
  "1,\pounds,N1,CT2,N1,CT3,N1"
]
\end{lstlisting}
\end{minipage}

\paragraph{Which is the best implemented reward function?}

Each of the basic reward functions (\textit{dwt, as, ql, dql, p}) have been used to train an agent with the same dataset and seeds.
These configurations have been then evaluated with five runs each (using the same seed for each configuration but a different one for each run) and the results have been collected and analyzed.

As can be seen in Figure \ref{fig:exp-1-rew-b}, which summarizes the results in a radar plot, the \textit{DiffWaitingTime} (dwt) reward function excels among the other because it grants lower (accumulated) waiting times and is among the highest for average speed, departure rate and arrival rate.
Meaning that more vehicles can use the intersection network, they can arrive faster at destination and have to wait less time.

In the last graph the AverageSpeed (as) reward function is missing, but for a good reason.
As can be seen in Figure \ref{fig:exp-1-rew-a}, AverageSpeed grants a higher average speed but leads to a way worse accumulated waiting time, meaning that at the end of their journey, those vehicles would have been waiting for at least double the time.
Since its performance are notably worse, to increase the readibility of the previous plot, this component has been removed.

\putimage{figures/exp/exp-1-rew-b.png}{Comparison of DiffWaitingTime (dwt), QueueLength (ql), DiffQueueLength (dql) and Pressure (p) reward functions.}{fig:exp-1-rew-b}{0.75}

\putimage{figures/exp/exp-1-rew-a.png}{Comparison of DiffWaitingTime (dwt), AverageSpeed (as), QueueLength (ql), DiffQueueLength (dql) and Pressure (p) reward functions.}{fig:exp-1-rew-a}{0.75}

\paragraph{Which is the best implemented observation function?}

Each of the basic observation functions (\textit{default, s, d, q}) have been used to train an agent with the same dataset and seeds. These configurations have been then evaluated with five runs each (using the same seed for each configuration but a different one for each run) and the results have been collected and analyzed.

As can be seen in Figure \ref{fig:exp-1-obs-b}, the \textit{Density} (d) observation function excels among the other because it grants lower accumulated waiting times and is among the highest for average speed, departure rate and arrival rate.
The only metric in which seems to be worse than the alternatives is the mean waiting time: vehicles are waiting in average $\sim2$ seconds more at full-stop.
This however doesn't hold for the accumulate waiting time.
This means that drivers may feel to be waiting less but in average their travel time is higher by $\sim10$ seconds.
Both the metric are important to evaluate how much a driver get stuck in traffic jams, since a lower waiting time score may means an higher frequency acceleration-deceleration cycles.
Every time that a vehicle moves, the waiting time metric get zeroed, since it only measures how much time a driver has been standing still in a precise moment in time.

In the last graphs the Default observation function is missing, but for a good reason.
As can be seen in Figure \ref{fig:exp-1-obs-a}, Default grants a higher average speed but leads to a way worse accumulated waiting time, meaning that at the end of their journey, those vehicles would have been waiting for at least double the time.
Since its performance are notably worse, to increase the readibility of the previous plot, this component has been removed.
A possible reason to explain this phenomenon is the fact that the state space size of the Default function is way bigger than the rest, thus increasing the sparsity of state information and making it more difficult to capture relevant features.

\putimage{figures/exp/exp-1-obs-b.png}{Comparison of Speed (s), Density (d) and Queue (q) observation functions.}{fig:exp-1-obs-b}{0.75}

\putimage{figures/exp/exp-1-obs-a.png}{Comparison of Default (default), Speed (s), Density (d) and Queue (q) observation functions.}{fig:exp-1-obs-a}{0.75}

\paragraph{Conclusions}

Using the \textit{DiffWaitingTime} reward function in combination with the \textit{Density} observation function allows to reach a good compromise between average vehicle speed and waiting times.
Notably, every \textit{"winning"} option is among the ones with slower average speed, but even the lowest value ($\sim9$) is similar or higher than the average speed in the major metropolitan areas ($\sim 6 m/s \; = \; 20 km/h$).

\section{Evaluating the effectiveness of Curriculum Learning}

Using a proper training dataset is at least as important as choosing the right reward and observation functions.
So far a monolithic dataset have been used in experiments.
It is constructed so that the agent while training it can experience all possible types of real-world traffic imaginable.
Such dataset can be assemble more easily with synthetic data rather than with real data because it would take a very long time for every single possibility to happear and it would be necessary to shred the collected data to synthesize a complete tape.
Moreover, when the network scales from a few intersections to hundreds or thousands of intersections, maybe with different shape and importance, a monolithic approach will suffer of Curse of Dimensionality.

Curriculum learning can applied in this context to simplify dataset construction and hopefully enhancing the agent learning process.
The \textbf{Curriculum Daily} dataset contains four episodes of daily traffic which represent the regular traffic on workday (2) and weekdays (2) without any kind of artificial impairments.
The \textbf{Curriculum Daily + Disruption} dataset follows the same principle as the last dataset but replaces two regular traffic episodes with two episodes in which there is disruption.
Their schemas are reported in Listings \ref{lst:dataset-schema-curriculum-daily} and \ref{lst:dataset-schema-curriculum-daily-plus-disruption}.

\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[language=JSON, caption=The schema for the "Curriculum Daily" Dataset, label={lst:dataset-schema-curriculum-daily}, mathescape=true]
"training": [
  "2,$\pounds$,N1,N2,N1,N3,N1",
  "2,$\pounds$,N1,N4,N1,N5,N1"
], "evaluation": [
  "1,$\pounds$,N1,N2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,N5,N1",
  "1,$\pounds$,N1,STPL2,N1,STPL3,N1",
  "1,$\pounds$,N1,CT2,N1,CT3,N1"
]
\end{lstlisting}
\end{minipage}

\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[language=JSON, caption=The schema for the "Curriculum Daily + Disruption" Dataset, label={lst:dataset-schema-curriculum-daily-plus-disruption}, mathescape=true]
"training": [
  "1,$\pounds$,N1,N2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,N5,N1",
  "1,$\pounds$,N1,ST2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,CT5,N1"
], "evaluation": [
  "1,$\pounds$,N1,N2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,N5,N1",
  "1,$\pounds$,N1,STPL2,N1,STPL3,N1",
  "1,$\pounds$,N1,CT2,N1,CT3,N1"
]
\end{lstlisting}
\end{minipage}

Experiment results (shown in Figure \ref{fig:exp-2-dat}) suggest that the \textbf{Curriculum Daily + Disruption} dataset grants a lower average (accumulated) waiting time while increasing the throughput with higher arrival/departure rates, all at the cost of a lower average vehicle speed. Moreover, an inspection of the metric plots over time revealed that the performances of this dataset are more stable than the ones of the \textbf{Curriculum Daily} dataset and of the \textbf{Monolithic} dataset.

Collected data supports the hypothesis that \textit{Curriculum Daily + Disruption} dataset balances the best of both other datasets.
The stability and easiness from the \textit{Curriculum Daily} dataset allows agent to incrementally learn without being permanently drown in negative rewards.
On the other hand, the presence of disruptions as in the \textit{Monolithic} dataset allows the agent to learn to adapt to tough traffic conditions.
Notably, the peak-hour disruptions supplied in the \textit{Curriculum Daily + Disruption} are the low-to-medium volume variants of their type.
This means that a curriculum learning driven dataset allows the agents to generalize more and learn a policy of higher quality (with respect to performance and stability) than a collage-like monolithic dataset.
As a consequence, real-world daily traffic data can be used for training without regret (with the assumption that disruptions often happen on a daily basis).

\putimage{figures/exp/exp-2-dat.png}{Comparison of Monolithic, Curriculum Daily and Curriculum Daily + Disruption datasets.}{fig:exp-2-dat}{0.75}

% Does curriculum learning leads to better running performances? Yes, it can generalize more and is more stable
% Is there a loss of knowledge due to curriculum learning not being exhaustive? Probably no
% Does adding disruption episodes in the training set enable agents to tackle conditions? Yes, its lack leads to a loss of knowledge

\section{Comparing Deep Learning with Tabular Learning}

The QLAgent used so far is an example of tabular method for reinforcement learning.
In Chapter 3 more reinforcement learning have been presented and now the interest is in comparing \textit{simpler} \textbf{tabular} methods with the more \textit{sophisticated} \textbf{neural} methods.
In order to evaluate the potential of those approaches, two distinct experiments were conducted comparing the different tabular/neural agents available on the same simulation conditions.
In both the experiments, the dataset being used is \textit{Curriculum Daily + Disruption}, the observation function is \textit{Density} and the reward function is \textit{DiffWaitingTime}.

\paragraph{Which is the best implemented tabular agent?}

As can be seen in Figure \ref{fig:exp-3-tab}, the QL and DQL agents show a clear advantage over the other model in terms of average speed, throughput (arrival/departure rate) and accumulated waiting time.
In fact, their metrics are very similar: the tie is break using the accumulated waiting time metric in which the DQL agent is $1$ second faster.
However, looking at waiting times, the SARSA agent is in average $2$ seconds faster than the Off Policy agents.
Its slower score record in accumulated waiting time is probably due to SARSA's tendency to favor a faster pace experience over long run waiting time.

\putimage{figures/exp/exp-3-tab.png}{Comparison of QL, SARSA and DQL agents.}{fig:exp-3-tab}{0.75}

\paragraph{Which is the best implemented neural agent?}

Figure \ref{fig:exp-3-neu} shows that the PPO agent bulldozes the DQN agent in almost every metric.
Interestingly, the shape of the DQN curve in this graph is similar to the one of the worst reward/observation functions (Figures \ref{fig:exp-1-rew-a} and \ref{fig:exp-1-obs-a}).
The only difference is that the underlying situation is very different.
SUMO-RL allows researches to perform a graphical demo in which the agents and their configuration can be seen in action.
A quick inspection using the demo mode revealed that the DQN agent wasn't learning at all.
Instead, all the intersections where stuck with green light in one direction during the entirety of the simulation.
A possible explanation is that a DQN model would need more training time to reach the performances of the other neural model.
This hypothesis is supported with empirical evidence in the field of reinforcement learning \cite{kozlica2023deep}.

\putimage{figures/exp/exp-3-neu.png}{Comparison of DQN and PPO agents.}{fig:exp-3-neu}{0.75}

\paragraph{Is Deep Learning worth its cost?}

Figure \ref{fig:exp-3-tot} summarizes the results of the experiments being carried out and shows that, given the same training time and conditions, a sophisticated (yet simple) tabular agent based on Q Learning can easily outperform one of the most popular and powerful neural network algorithm for reinforcement learning.
Of course, on the long run the PPO agent \textbf{may} be able to gain ground on DQL with more training episodes, but the use of conditional is needed and the empirical evidence \cite{kozlica2023deep} suggests that usually PPO has a initial steep learning curve and after some time it comes into a plateau, just as tabular methods.

\putimage{figures/exp/exp-3-tot.png}{Comparison of DQL and PPO agents.}{fig:exp-3-tot}{0.75}

\section{Comparing Reinforcement Learning with existing solutions}

Now that there is a clearer picture of the potential of the main reinforcement learning algorithms implemented in SUMO-RL, it is vital to understand if they stand a chance with fixed cycle traffic light programs.

\paragraph{What is the impact of cycle time in Fixed Cycle Agents?}

In order to compare RL agents to a fixed cycle agent, it is important to select the best performing cycle time for it to work with. Figure \ref{fig:exp-4-fca} shows a comparison of four possible cycle times which are widely used in urban controlled intersections.
A singular pattern can be noted in agent shapes: the lower is cycle time, the better are performance ratings.
This is true for almost all the metrics but the average speed, in which this rule appears to be inverted.

A logical explanation for this phenomenon is that with lower cycle time, vehicles are waiting less time at full-stop at intersection entrance.
Interestingly, this holds also for accumulated waiting time, in which there is a clear advantage for the lower cycle times (15 and 30).
However, a faster pace traffic light also means that there are frequent acceleration and decelerations, therefore the average speed can be expected to be higher in long cycle times (30, 45, 60).
Notably, the is an hidden threshold between $15$ seconds and $30$ seconds for which performance is subject to a radical change.

\putimage{figures/exp/exp-4-fca.png}{Comparison of Fixed15, Fixed30, Fixed45 and Fixed60 agents.}{fig:exp-4-fca}{0.75}

\paragraph{Are RL Agents an improvement over non-AI solutions?}

Now that the best configuration for cycle time is known, an experiment was run to compare two RL agents (PPO and DQL) with the best FixedCycle agent (Fixed15) and with unregulated intersection.
The reasoning behind the choice of running also a priority-based intersection is that in urban tissues, not all the busy intersections are regulated with traffic light, therefore the aim is to experimentally verify which is impact of installing a traffic light on such intersections. The results of this experiment are shown in Figure \ref{fig:exp-4-tot}.

The relative inefficiency of unregulated intersections is well-known and it is probably the least interesting suggestion of the radar plot.
Interestingly, the performances of the PPO agent are very similar to the ones of a traffic light intersection with a cycle time of $15$ seconds.
This not only shows the limit of efficiency of traffic lights with fixed cycle program, but also shows that at least the PPO agent has learnt a comparable policy.

Excluding the instant waiting time, in which its average value is very close to PPO and Fixed15 agents, the DQL agent surpasses by far its competitors in every possible ways.
Notably, the cost in terms of secondary memory of this agent is roughly $4$ MB, which means that it can be fitted into dedicated boards very easily.

\putimage{figures/exp/exp-4-tot.png}{Comparison of Fixed15, PPO and DQL agents and priority based intersections.}{fig:exp-4-tot}{0.75}

\section{Evaluating the impact of Determinism over performances}

All the reinforcement learning agents implemented use some kind of non-determinism mechanism in order to randomize a small share of the action that would be chosen. Tabular agents use an Epsilon Greedy curve which yields totally random actions, while neural agents interpret their output as a probability distribution, choosing an action with the high expected reward with high probability and an action with low expected reward with low probability. The choice of allowing non-determinism to partially dominate their behaviour is not obvious and the interest here is to discover if there is a tangible effect on performance and then if the agents are benefiting from it. Two separate experiments have been conducted to answer to these questions.

The DQL agent has been tested with different values for the minimum $\epsilon$ variable, which results are shown in Figure \ref{fig:exp-5-dql}. There is no evidence for significant change over minimum $\epsilon$ value other than little improvements, but there seem to be an hidden threshold for which one of the variants ($0.01$) is significantly worse than the others ($0.05, 0.005, 0.001$). A possible explanation is that the worst value lies in a unlucky middle zone in which values are too noisy to stabilize a traffic light intersection but not noisy enough to produce a significant exploration of the state-action-reward space, meaning less learning power.

The PPO agent has been tested with non-determinism disabled only during evaluation. Disabling it also in the training process would be unfair since reinforcement learning models often benefit from the state-action-reward space exploration. The results are shown in Figure \ref{fig:exp-5-ppo}. Empirical evidence shows that the PPO is too weak to survive without using the probability distribution trick. This is not necessarely a problem: by theory, if the Q table approximation is \textit{correct}, the maximum average expected reward achievable is gained by following such a probabilistic policy. The problem raises in comparison with tabular agents, because even with very low residual $\epsilon$ they achieve a decent performance.

Motivated by this performance drops, \textit{stable\_baselines} PPO implementation was searched for other hyperparameters which may determine the ability of a neural model to learn better.
An example is the buffer size, which capacity is inversely proportional to the number of learning updates since usually learning is triggered whenever this buffer is filled.
Another example is the entropy regularization coefficient, which is used to enact an optimal trade-off between exploration and exploitation, being essentially an equivalent in \textit{stable\_baselines} of the Epsilon Greedy algorithm of tabular methods.
Further experiments have been executed to evaluate their impact, however they have revealed that these changes have no significant effects and that hyperparameter tuning is unlikely to boost PPO performance.

\putimage{figures/exp/exp-5-dql.png}{Comparison of different minimum values for $\epsilon$ (Epsilon-Greedy).}{fig:exp-5-dql}{0.75}
\putimage{figures/exp/exp-5-ppo.png}{Comparison of disabling/enabling non-determinism in the PPO agent.}{fig:exp-5-ppo}{0.75}

\section{Evaluating the impact of Quantization over performances}

State quantization is a very useful tecnhique because it allows to reduce a continuous space into a discrete space. However, choosing the right number of quantization levels is essential as a excessively small quantization space would lead to loss of information while with a oversized space it would be impossible to have any significant positive effect.

To verify the presence of better quantization values other than the default one ($16$), the DQL agent has been trained and used in a experiment with different quantization values ($8, 16, 32, 64$), and the results are shown in Figure \ref{fig:exp-6-dql}.
Evidence shows that the agent is profiting from the reduction of state space, as expected, and in particular there is a hidden threshold between $8$ and $16$ in which benefits get spread more on throughput than in waiting times.
Notably, each reduction of state space leads to a decrease of average space in favor of better waiting time and/or throughput, as happened in other experiments, and the value of $16$ is confirmed to be an optimal trade-off.

Since the PPO agent by theory is suitable also for continuous state spaces, the same type of experiment has been carried out also for this agent, which results are shown in Figure \ref{fig:exp-6-ppo}.
Interestingly, PPO takes no significant advantage or damage from using a higher/lower quantization or not using quantization at all.

%% MOTIVATES TO INVESTIGATE DIFFERENT NETWORK SIZES

\putimage{figures/exp/exp-6-dql.png}{Comparison of 8, 16, 32, 64 quantization levels for the DQL agent.}{fig:exp-6-dql}{0.75}
\putimage{figures/exp/exp-6-ppo.png}{Comparison of 8, 16, 32, 64 quantization levels and continuous-space for the PPO agent.}{fig:exp-6-ppo}{0.75}

\section{Comparing Multi-Agent Learning with Single-Agent Learning}

Usually modelling a reinforcement learning task as a single-agent activity simplifies the design and implementation of a RL system.
Even when there is a multi-agent system, an agent is usually thought to observe only a portion of the world directly under its control \cite{alegre2021quantifying}.
In the experiment run so far, the agents can observe only their internal state or some features of their incoming/outcoming lanes, and they are rewarded only for the quality of their intersection dynamic program.
However, each intersection is part of a road network, and as such it influences it (and it is influenced back) though its behavior.
Each agent may be pursuing a local optimum behavior, but if there is no coordination nor collaboration, at global level the system could fail.

To solve this problem there are mainly two types of approaches: (1) letting an agent to control and observe more than one intersection at the same time and (2) letting an agent to observe neighbour/global zones and to perceive neighbour/global rewards.
While the first technique has been used by other works in RL literature on traffic lights \cite{wei2019presslight}, the second one is thought to be more interesting since it requires less sophisticated agents and it is more realistic with respect to how alive creatures (especially in collaborative societies) experience learning and tasks. Think for example to a soccer game.
Intuitively, an intersection benefits from a good behaviour of their neighbours just as a soccer players benefits from playing as a team, in which also the least talented one still wins if a more talented teammate scores.
Moreover, an intersection willing to observe its surrounding not to impede neighbour intersection follows the same principle as two players synchronising with eyesight for a ball passage.
In the present section the shared-view mechanisms introduced in the last chapter will be used to let traffic lights coordinate and learn from collective ownership of rewards and observations.

\paragraph{Does sharing rewards to neighbour agents improve globally the solution?}

In Figure \ref{fig:exp-7-rew} are shown the results of an experiment in which the agent DQL has been tested with the baseline DiffWaitingTime reward function and with the shared-view reward functions presented in the previous chapter.
As expected, the the traffic light system gains a global advantage of $-5$ seconds in accumulated waiting time and $-2$ seconds in instant waiting time by sharing the rewards of the DiffWaitingTime function in the neighbourhood.
Notably, excluding two aforementioned functions and the shared-view of DiffQueueLength which achieves similar performances to the Fixed15 agent, all the other performance ratings are worse than the PPO agent.
Interestingly, they are actually equal or worse than the original non-shared-view functions they used as base.
This evidence supports the idea that sharing the rewards of a function with mediocre performance is probably going to create a worthless reward function which is actually counterproductive.
Finally, a further experiment as revealed that there is no significant difference in summing neighbours' raw rewards or dumping them with a scaling factor.
Obviously such shared-view construct and in general this last consideration only makes sense if the same reward function is used for neighbourhood and for the agent iself, because otherwise rewards coming from different sources should be rescaled to match in domain.


\paragraph{Does sharing observations to neighbour agents improve globally the solution?}

In Figure \ref{fig:exp-7-obs} are shown the results of an experiment in which the agent DQL has been tested with the baseline Density observation function and with the shared-view observation functions presented in the previous chapter (recall that with this shared-view all agents are receiving a concatenation of the baseline and the shared-view features).
It is observed that almost all shared-view function worsen the performance with respect of the base function.
However, using the observation function that share the current phase of traffic lights to neighbours, there is a significant gain in terms of waiting time, throughput and average speed.

\newpage
\putimage{figures/exp/exp-7-rew.png}{Comparison of DiffWaitingTime and the shared-view reward functions for the DQL agent.}{fig:exp-7-rew}{0.65}

\putimage{figures/exp/exp-7-obs.png}{Comparison of Density and the shared-view observation functions for the DQL agent.}{fig:exp-7-obs}{0.65}

% In Figure \ref{fig:exp-7-par} are shown the results of an experiment in which the controllable intersection have been distributed to a pool of agents with different types of partitioning.
% 
% \putimage{figures/exp/exp-7-par.png}{Comparison of intersection partitioning techniques for Few-Agent/Single-Agent learning with the DQL agent.}{fig:exp-7-par}{0.75}

\section{Evaluating the effectiveness of Self-Adaptive strategies}

Traffic control system cannot be thought as an immutable object which, once trained, will stay still forever.
Traffic habits change and so do demand, usually driven by socio-economic factors but also by perception change due to drivens experiencing more or less traffic jams over time.
Ideally, the system could need to be retrained with updated dataset every year, if not every month with small learning slots.
What if a reinforcement learning system is able to detect automatically degradation of performance and to schedule a small training session when needed so that is gets stronger over time?
That's exacly what has been tried applying the Self-Adaptive algorithm explained in the last chapter.
As usual, the DQL has been the laboratory rat with the best non-shared-view configuration available, and the results of the experiment are shown in Figure \ref{fig:exp-8-sam}.
Evidence shows that such mechanism would instead worsen performance by a significant amount in all statistics.
Since the threshold used in triggering a learning slot is very strict, the doubt remained whether it was actually used but an inspection of simulation log has found to be case.
Therefore, an explanation of this phenomenon is that learning is not a stable process, fluctuations due to Q table updates probably have induced unwanted behavior and therefore worse performance ratings.

\putimage{figures/exp/exp-8-sam.png}{Comparison of DQL agent performance with Self-Adaptive enabled/disabled.}{fig:exp-8-sam}{0.75}
