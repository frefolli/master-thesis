\chapter{Experiments and findings}
%% DATASETS ARE GENERATED WITH SEED = 170701

\section{RO 1 - Acquiring a baseline}
  
  Before even evaluating novel approaches in Reinforcement Learning it is useful to investigate which are the best base settings. This involves finding the best Reward function and the best Observation function among the implemented set of functions. To optimize these two hyperparameters, two distinct experiments have been performed in parallel, each one comparing the various implemented functions.

  The QLAgent has been trained for $400k$ virtual seconds with a \textbf{monolithic} dataset containing each one of the entries of the Traffic Registry. It is then evaluated for another $400k$ virtual seconds using an evaluation dataset composed of four possible everyday scenarios comprising both low traffic and high congestions. From a technical standpoint, this complete dataset has been generated with the following schema using the \textit{generate-flow/dataset} utility:

\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[language=JSON, caption=The schema for the "Monolithic" Dataset, label={lst:dataset-schema-monolithic}, mathescape=true]
"training": [
  "4,100000,$\pounds$,*,$\sim$"
], "evaluation": [
  "1,\pounds,N1,N2,N1,N3,N1",
  "1,\pounds,N1,N4,N1,N5,N1",
  "1,\pounds,N1,STPL2,N1,STPL3,N1",
  "1,\pounds,N1,CT2,N1,CT3,N1"
]
\end{lstlisting}
\end{minipage}

\subsection{RQ 1.1 - Which is the best implemented reward function?}

Each of the basic reward functions (\textit{dwt, as, ql, dql, p}) have been used to train an agent with the same dataset and seeds.
These configurations have been then evaluated with five runs each (using the same seed for each configuration but a different one for each run) and the results have been collected and analyzed.

As can be seen in Figure \ref{fig:exp-1-rew-b}, which summarizes the results in a radar plot, the \textit{DiffWaitingTime} (dwt) reward function excels among the other because it grants lower (accumulated) waiting times and is among the highest for average speed, departure rate and arrival rate.
Meaning that more vehicles can use the intersection network, they can arrive faster at destination and have to wait less time.

\putimage{figures/exp/exp-1-rew-b.png}{Comparison of DiffWaitingTime (dwt), QueueLength (ql), DiffQueueLength (dql) and Pressure (p) reward functions.}{fig:exp-1-rew-b}{1.0}

In the above graph the AverageSpeed (as) reward function is missing, but for a good reason.
As can be seen in Figure \ref{fig:exp-1-rew-a}, AverageSpeed grants a higher average speed but leads to a way worse accumulated waiting time, meaning that at the end of their journey, those vehicles would have been waiting for at least double the time.
Since its performance are notably worse, to increase the readibility of the previous plot, this component has been removed.

\putimage{figures/exp/exp-1-rew-a.png}{Comparison of DiffWaitingTime (dwt), AverageSpeed (as), QueueLength (ql), DiffQueueLength (dql) and Pressure (p) reward functions.}{fig:exp-1-rew-a}{1.0}

\subsection{RQ 1.2 - Which is the best implemented observation function?}

Each of the basic observation functions (\textit{default, s, d, q}) have been used to train an agent with the same dataset and seeds. These configurations have been then evaluated with five runs each (using the same seed for each configuration but a different one for each run) and the results have been collected and analyzed.

As can be seen in Figure \ref{fig:exp-1-obs-b}, the \textit{Density} (d) observation function excels among the other because it grants lower accumulated waiting times and is among the highest for average speed, departure rate and arrival rate.
The only metric in which seems to be worse than the alternatives is the mean waiting time: vehicles are waiting in average $\sim2$ seconds more at full-stop.
This however doesn't hold for the accumulate waiting time.
This means that drivers may feel to be waiting less but in average their travel time is higher by $\sim10$ seconds.
Both the metric are important to evaluate how much a driver get stuck in traffic jams, since a lower waiting time score may means an higher frequency acceleration-deceleration cycles.
Every time that a vehicle moves, the waiting time metric get zeroed, since it only measures how much time a driver has been standing still in a precise moment in time.

\putimage{figures/exp/exp-1-obs-b.png}{Comparison of Default (default), Speed (s), Density (d) and Queue (q) observation functions.}{fig:exp-1-obs-b}{1.0}

In the above graphs the Default observation function is missing, but for a good reason.
As can be seen in Figure \ref{fig:exp-1-obs-a}, Default grants a higher average speed but leads to a way worse accumulated waiting time, meaning that at the end of their journey, those vehicles would have been waiting for at least double the time.
Since its performance are notably worse, to increase the readibility of the previous plot, this component has been removed.
A possible reason to explain this phenomenon is the fact that the state space size of the Default function is way bigger than the rest, thus increasing the sparsity of state information and making it more difficult to capture relevant features.

\putimage{figures/exp/exp-1-obs-a.png}{Comparison of Speed (s), Density (d) and Queue (q) observation functions.}{fig:exp-1-obs-a}{1.0}

\subsection{Conclusions}

Using the \textit{DiffWaitingTime} reward function in combination with the \textit{Density} observation function allows to reach a good compromise between average vehicle speed and waiting times.
Notably, every \textit{"winning"} option is among the ones with slower average speed, but even the lowest value ($\sim9$) is similar or higher than the average speed in the major metropolitan areas ($\sim 6 m/s \; = \; 20 km/h$).

\section{RO 2 - Evaluating the effectiveness of Curriculum Learning}

Using a proper training dataset is at least as important as choosing the right reward and observation functions.
So far a monolithic dataset have been used in experiments.
It is constructed so that the agent while training it can experience all possible types of real-world traffic imaginable.
Such dataset can be assemble more easily with synthetic data rather than with real data because it would take a very long time for every single possibility to happear and it would be necessary to shred the collected data to synthesize a complete tape.
Moreover, when the network scales from a few intersections to hundreds or thousands of intersections, maybe with different shape and importance, a monolithic approach will suffer of Curse of Dimensionality.

Curriculum learning can applied in this context to simplify dataset construction and hopefully enhancing the agent learning process.
The \textbf{Curriculum Daily} dataset contains four episodes of daily traffic which represent the regular traffic on workday (2) and weekdays (2) without any kind of artificial impairments.
The \textbf{Curriculum Daily + Disruption} dataset follows the same principle as the last dataset but replaces two regular traffic episodes with two episodes in which there is disruption.
Their schemas are reported in Listings \ref{lst:dataset-schema-curriculum-daily} and \ref{lst:dataset-schema-curriculum-daily-plus-disruption}.

\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[language=JSON, caption=The schema for the "Curriculum Daily" Dataset, label={lst:dataset-schema-curriculum-daily}, mathescape=true]
"training": [
  "2,$\pounds$,N1,N2,N1,N3,N1",
  "2,$\pounds$,N1,N4,N1,N5,N1"
], "evaluation": [
  "1,$\pounds$,N1,N2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,N5,N1",
  "1,$\pounds$,N1,STPL2,N1,STPL3,N1",
  "1,$\pounds$,N1,CT2,N1,CT3,N1"
]
\end{lstlisting}
\end{minipage}

\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[language=JSON, caption=The schema for the "Curriculum Daily + Disruption" Dataset, label={lst:dataset-schema-curriculum-daily-plus-disruption}, mathescape=true]
"training": [
  "1,$\pounds$,N1,N2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,N5,N1",
  "1,$\pounds$,N1,ST2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,CT5,N1"
], "evaluation": [
  "1,$\pounds$,N1,N2,N1,N3,N1",
  "1,$\pounds$,N1,N4,N1,N5,N1",
  "1,$\pounds$,N1,STPL2,N1,STPL3,N1",
  "1,$\pounds$,N1,CT2,N1,CT3,N1"
]
\end{lstlisting}
\end{minipage}

Experiment results (shown in Figure \ref{fig:exp-2-dat}) suggest that the \textbf{Curriculum Daily + Disruption} dataset grants a lower average (accumulated) waiting time while increasing the throughput with higher arrival/departure rates, all at the cost of a lower average vehicle speed. Moreover, an inspection of the metric plots over time revealed that the performances of this dataset are more stable than the ones of the \textbf{Curriculum Daily} dataset and of the \textbf{Monolithic} dataset.

Collected data supports the hypothesis that \textit{Curriculum Daily + Disruption} dataset balances the best of both other datasets.
The stability and easiness from the \textit{Curriculum Daily} dataset allows agent to incrementally learn without being permanently drown in negative rewards.
On the other hand, the presence of disruptions as in the \textit{Monolithic} dataset allows the agent to learn to adapt to tough traffic conditions.
Notably, the peak-hour disruptions supplied in the \textit{Curriculum Daily + Disruption} are the low-to-medium volume variants of their type.
This means that a curriculum learning driven dataset allows the agents to generalize more and learn a policy of higher quality (with respect to performance and stability) than a collage-like monolithic dataset.
As a consequence, real-world daily traffic data can be used for training without regret (with the assumption that disruptions often happen on a daily basis).

\putimage{figures/exp/exp-2-dat.png}{Comparison of Monolithic, Curriculum Daily and Curriculum Daily + Disruption datasets.}{fig:exp-2-dat}{1.0}

% Does curriculum learning leads to better running performances? Yes, it can generalize more and is more stable
% Is there a loss of knowledge due to curriculum learning not being exhaustive? Probably no
% Does adding disruption episodes in the training set enable agents to tackle conditions? Yes, its lack leads to a loss of knowledge

\section{RO 3 - Comparing Deep Learning with Tabular Learning}

The QLAgent used so far is an example of tabular method for reinforcement learning.
In Chapter 3 more reinforcement learning have been presented and now the interest is in comparing \textit{simpler} \textbf{tabular} methods with the more \textit{sophisticated} \textbf{neural} methods.
In order to evaluate the potential of those approaches, two distinct experiments were conducted comparing the different tabular/neural agents available on the same simulation conditions.
In both the experiments, the dataset being used is \textit{Curriculum Daily + Disruption}, the observation function is \textit{Density} and the reward function is \textit{DiffWaitingTime}.

\subsection{RQ 3.1 - Which is the best implemented tabular agent?}

As can be seen in Figure \ref{fig:exp-3-tab}, the QL and DQL agents show a clear advantage over the other model in terms of average speed, throughput (arrival/departure rate) and accumulated waiting time.
In fact, their metrics are very similar: the tie is break using the accumulated waiting time metric in which the DQL agent is $1$ second faster.
However, if we look at waiting times, the SARSA agent is in average $2$ seconds faster than the Off Policy agents.
Its slower score record in accumulated waiting time is probably due to SARSA's tendency to favor a faster pace experience over long run waiting time.

\putimage{figures/exp/exp-3-tab.png}{Comparison of QL, SARSA and DQL agents.}{fig:exp-3-tab}{1.0}

\subsection{RQ 3.2 - Which is the best implemented neural agent?}

\putimage{figures/exp/exp-3-neu.png}{Comparison of DQN and PPO agents.}{fig:exp-3-neu}{1.0}

\subsection{RQ 3.3 - Is Deep Learning worth its cost?}

\putimage{figures/exp/exp-3-tot.png}{Comparison of DQL and PPO agents.}{fig:exp-3-tot}{1.0}

\section{RO 4 - Comparing Reinforcement Learning with existing solutions}

\subsection{RQ 4.1 - What is the impact of cycle time in Fixed Cycle Agents?}

\putimage{figures/exp/exp-4-tot.png}{Comparison of Fixed15, Fixed30, Fixed45 and Fixed60 agents.}{fig:exp-4-fca}{1.0}

\subsection{RQ 4.2 - Are RL Agent an improvement over non-AI solutions?}

\putimage{figures/exp/exp-4-tot.png}{Comparison of Fixed15 and DQL agents and priority based intersections.}{fig:exp-4-tot}{1.0}
%
% Which cycle time grants better performance? fixed30
% Is fixed cycle traffic light an improvement over unregulated priority based intersections? Yes
% L'agente RL e' un miglioramento rispetto alle performance delle soluzioni (di base) correnti?
% Does the best RL agent represent an improvement with respect to performances achieved by current traffic regulation solutions? Yes

\section{RO 5 - Evaluating the impact of learning parameters over performances}
%   \subsection{E11: Explore Epsilon Greedy curve (DQL)}
%   \subsection{E12: Explore Buffer Size (PPO)}
\putimage{figures/exp/exp-5-buf.png}{}{fig:exp-5-buf}{1.0}

\section{RO 6 - Evaluating the impact of Determinism over performances}
%   \subsection{E11: Explore determinism (DQL)}
%   \subsection{E12: Explore determinism (PPO)}
\putimage{figures/exp/exp-6-dql.png}{}{fig:exp-6-dql}{1.0}
\putimage{figures/exp/exp-6-ppo.png}{}{fig:exp-6-ppo}{1.0}

\section{RO 7 - Evaluating the impact of Quantization over performances}
%   \subsection{E8: Try Unquantized (DQL)}
%   \subsection{E9: Try Unquantized (PPO)}
\putimage{figures/exp/exp-7-dql.png}{}{fig:exp-7-dql}{1.0}
\putimage{figures/exp/exp-7-ppo.png}{}{fig:exp-7-ppo}{1.0}

\section{RO 8 - Comparing Multi-Agent Learning with Single-Agent Learning}
%   \subsection{E5: Try Marl On Observation}
%   \subsection{E6: Try Marl On Reward}
%   \subsection{E10: Try Partition}
\putimage{figures/exp/exp-8-rew.png}{}{fig:exp-8-rew}{1.0}
\putimage{figures/exp/exp-8-obs.png}{}{fig:exp-8-obs}{1.0}
\putimage{figures/exp/exp-8-par.png}{}{fig:exp-8-par}{1.0}

\section{RO 9 - Evaluating the effectiveness of Self-Adaptive strategies}
%   \subsection{E4: Try Self Adaptive}
\putimage{figures/exp/exp-9-sam.png}{}{fig:exp-9-sam}{1.0}
